{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pythainlp in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied, skipping upgrade: tinydb>=3.0 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pythainlp) (4.3.0)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.22.0 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pythainlp) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: python-crfsuite>=0.9.6 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pythainlp) (0.9.7)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.22.0->pythainlp) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.22.0->pythainlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.22.0->pythainlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.22.0->pythainlp) (2019.11.28)\n",
      "Requirement already satisfied: pyLDAvis in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (2.1.2)\n",
      "Requirement already satisfied: pandas>=0.17.0 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.0.1)\n",
      "Requirement already satisfied: wheel>=0.23.0 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.36.2)\n",
      "Requirement already satisfied: pytest in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (5.3.5)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.18.1)\n",
      "Requirement already satisfied: joblib>=0.8.4 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.14.1)\n",
      "Requirement already satisfied: funcy in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.15)\n",
      "Requirement already satisfied: future in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.4.1)\n",
      "Requirement already satisfied: numexpr in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (2.7.1)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (2.11.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2019.3)\n",
      "Requirement already satisfied: py>=1.5.0 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (1.8.1)\n",
      "Requirement already satisfied: packaging in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (20.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (19.3.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (8.2.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.13.1)\n",
      "Requirement already satisfied: wcwidth in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.1.8)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (1.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.17.0->pyLDAvis) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from packaging->pytest->pyLDAvis) (2.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: emoji in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: wordcloud in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (1.8.1)\n",
      "Requirement already satisfied: pillow in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from wordcloud) (7.0.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from wordcloud) (1.18.1)\n",
      "Requirement already satisfied: matplotlib in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from wordcloud) (3.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->wordcloud) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->wordcloud) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (46.0.0.post20200309)\n",
      "\n",
      "Usage:   \n",
      "  pip uninstall [options] <package> ...\n",
      "  pip uninstall [options] -r <requirements file> ...\n",
      "\n",
      "no such option: -U\n",
      "Requirement already up-to-date: scikit-learn in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: marisa-trie in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (0.7.5)\n",
      "Requirement already up-to-date: pythainlp in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.22.0 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pythainlp) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: tinydb>=3.0 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pythainlp) (4.3.0)\n",
      "Requirement already satisfied, skipping upgrade: python-crfsuite>=0.9.6 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from pythainlp) (0.9.7)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.22.0->pythainlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.22.0->pythainlp) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.22.0->pythainlp) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /Users/abcd/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.22.0->pythainlp) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pythainlp\n",
    "!pip install pyLDAvis\n",
    "!pip install emoji\n",
    "!pip install wordcloud\n",
    "!pip uninstall -U scikit-learn\n",
    "!pip install -U scikit-learn\n",
    "!pip install marisa-trie\n",
    "!pip install pythainlp -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "import pythainlp\n",
    "from pythainlp.corpus.common import thai_words\n",
    "from pythainlp.tokenize import word_tokenize, Tokenizer\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "#import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.util.trie import Trie\n",
    "from pythainlp.util import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"./Shopee_Tweet_NEW.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Create_at</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>ใช่ค่ะ ตอนแรกบอกว่าให้กดรับของได้เลยแล้วจะส่งใ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>อยากให้เรื่องนี้แมส เผื่อว่า จะออกมารับผิดชอบอ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-06</td>\n",
       "      <td>ขอบคุณบริษัทช้อปปี้มากฮะที่แอดมินพาพี่มายกับอา...</td>\n",
       "      <td>pos</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-26</td>\n",
       "      <td>บอกลาบริษัทช้อปปี้ก่อนนะ แอดมินแบบนี้ ซื้อด้วย...</td>\n",
       "      <td>neg</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>เรากดสั่งของไปรอของมาจนวันนี้ ไม่ส่งซักที วันน...</td>\n",
       "      <td>neg</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Create_at                                         clean_text Sentiment  \\\n",
       "0 2021-01-05  ใช่ค่ะ ตอนแรกบอกว่าให้กดรับของได้เลยแล้วจะส่งใ...       neg   \n",
       "1 2021-01-05  อยากให้เรื่องนี้แมส เผื่อว่า จะออกมารับผิดชอบอ...       neg   \n",
       "2 2021-02-06  ขอบคุณบริษัทช้อปปี้มากฮะที่แอดมินพาพี่มายกับอา...       pos   \n",
       "3 2021-03-26  บอกลาบริษัทช้อปปี้ก่อนนะ แอดมินแบบนี้ ซื้อด้วย...       neg   \n",
       "4 2021-02-01  เรากดสั่งของไปรอของมาจนวันนี้ ไม่ส่งซักที วันน...       neg   \n",
       "\n",
       "    Topics  \n",
       "0  Company  \n",
       "1  Company  \n",
       "2  Company  \n",
       "3  Company  \n",
       "4  Company  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Create_at\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_text    0\n",
       "Sentiment     0\n",
       "Topics        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add word in dict\n",
    "custom_dict = set(thai_words())\n",
    "\n",
    "Company_ = 'บริษัทช้อปปี้ แอดมิน คอลเซ็นเตอร์ เจ้าหน้าที่ พนักงาน บริษัท ค่าบริการ รับผิดชอบ ติดต่อ โกงเงิน บัญชี ดำเนินการ แบ่งชนชั้น คุยกับพนักงาน ติดต่อ บริการ แจ้ง ระดับ พรีเมียม เมมเบอร์ แบน เลิกใช้ สื่อสาร ร้องเรียน เลิกช้อป เว็บไซต์ เบอร์ โทร เมล สนใจ เพิกเฉย รอสาย ตัดสาย ด่วน เมล โทร ศูนย์บริการ ปัญหา ชดเชย องค์กร เลิกสั่ง ประทับใจ แนะนำ ชี้แจง นโยบาย สมาชิก การตลาด โฆษณา ลูกค้า ผู้ใช้บริการ แพลตฟอร์ม โฆษณา'\n",
    "Company_Topics = Company_.split(' ')\n",
    "\n",
    "Logistics_ = 'ไม่ติดต่อ ยกเลิก รอ ช้า นาน ชั่วโมง ไม่ได้รับ ไม่ได้สินค้า ปัญหา ผู้ส่ง พนักงาน ขนส่ง รอนาน บริการ ไม่โทรมา ส่ง ไม่มาส่ง ไม่ส่ง สุภาพ บอกทาง เชค เช็ค ตรวจ ที่อยู่ ส่งช้า ส่งเร็ว ส่งไว โทรมา ไม่โทร ไม่รับ เลือกขนส่ง เลือกขนส่งไม่ได้ เลือกบริษัทขนส่ง ได้ของ ส่งของ กำหนด บริษัทขนส่ง ค่าส่ง เก็บเงินปลายทาง เก็บปลายทาง เลขพัสดุ ค่าส่ง จ่าหน้า'\n",
    "Logistics_Topics = Logistics_.split(' ')\n",
    "\n",
    "Promotion_ =  'โค้ด โค๊ด โค้ดลด โค้ดช้อปปี้ กดไม่ทัน ใช้ไม่ทัน ค่าส่ง ลดราคา ส่วนลด โปรโมชั่น คุ้มค่า ไม่คุ้ม ถูก โปร โปรช้อปปี้ บอกต่อ ใช้ไม่ได้ ใช้โค้ดไม่ได้ คุณภาพ ใช้ได้ ขั้นต่ำ โค้ดส่งฟรี ส่งฟรี แกง ลูกค้า ใหม่ ฟรี แถม เต็ม กด ทัน เที่ยงคืน แฟลชเซลล์ เมมเบอร์ สิทธิ์ คอย คอยส์ แคมเปญ เหรียญ จ่ายบิล ดีล รดน้ำต้นไม้ คูปอง กิจกรรม'\n",
    "Promotion_Topics = Promotion_.split(' ')\n",
    "\n",
    "Store_ =  'ช้อป ซื้อ ซื้อมาจาก ร้านในช้อปปี้ ไลฟ์ ไลฟ์สด ในช้อปปี้ จากช้อปปี้ สั่งช้อปปี้ เข้าช้อปปี้ ที่ช้อปปี้ ราคา คุ้ม คุ้มค่า ไม่คุ้ม แพง ถูก ลด บอกต่อ คุ้ม คุณภาพ ซื้อ ส่วนต่าง ควรค่า ควรซื้อ ไปตำ แนะนำ ร้าน ร้านค้า พิกัด ร้าน รีวิว สั่ง สอย ซื้อจาก รวมของที่ซื้อ สินค้า ของ ขาย ขายของ บาท ใน จาก พร้อมส่ง คำสั่งซื้อ รายงานผู้ใช้ แม่ค้า พ่อค้า โกง ผู้ขาย แกง ร้านเสื้อผ้า เสียเงิน ขายที่ ขายดี หมดไว ปลอม'\n",
    "Store_Topics = Store_.split(' ')\n",
    "\n",
    "System_ = 'แอพ แอพช้อปปี้ แอพลิเคชั่น หน้าจอ รวน สมัคร ไถ ไถช้อปปี้ ชั่วคราว ระบบ เงิน เว็บ หน้าเว็บ ตัด บัตร แคนเซิล ยกเลิก เป็นอะไร พัง ค้าง ล่ม เสีย ไม่ได้ เด้ง รอ พัฒนา หักตังค์ หักเงิน คืนเงิน ตัดเงิน ตัดบัตร บัตร หักเงิน ใช้ ยาก ง่าย พัฒนา ไม่ตรง โอน โอนเงิน เครดิต คืน แก้ ด่วน ขัดข้อง ฟื้น แก้ไข จ่ายเงิน ผ่อน ปัญหา โดนระงับ ตรวจสอบ ขั้นตอน เงินไม่เข้า เติมเงิน ไม่เข้า จ่ายไม่ได้ แบบใหม่ แบบเก่า บัตรเครดิต บอท อัพเดต เงินคืน บัญชี อนุมัติ กดเงิน ช้อปปี้เพย์ แอร์เพย์ จอ อัตโนมัติ ฟีด โหมดดาร์ก สถานะ แฮ็ค ตะกร้า แจ้งเตือน ระงับ ยกเลิก คำสั่งซื้อ ตรวจสอบ ลิ้งค์ สะดวก ระบบล่ม เติม โหลด ปัดจอ โหลด เสิร์ช หมวด ธรรมเนียม ธุรกรรม วงเงิน วอลเล็ต'\n",
    "System_Topics = System_.split(' ')\n",
    "\n",
    "reviews_dict = Company_Topics+Logistics_Topics+Promotion_Topics+Store_Topics+System_Topics\n",
    "\n",
    "for word in reviews_dict:\n",
    "    custom_dict.add(word)\n",
    "\n",
    "\n",
    "trie = Trie(custom_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_correction_dict(incorrect_word,corrected):\n",
    "    if type(incorrect_word)==str:\n",
    "        correction_dict[incorrect_word]=corrected\n",
    "    else:\n",
    "        if type(incorrect_word)==tuple:\n",
    "            for word in incorrect_word:\n",
    "                correction_dict[word]=corrected\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_dict={}\n",
    "word_correction_dict(('บ','บอ','บ.'),'บริษัท')\n",
    "word_correction_dict(('call center','Call Center','คอลเซนเตอ','คอลเซนเตอร์'),'คอลเซ็นเตอร์')\n",
    "word_correction_dict(('พนง','Staff'),'พนักงาน')\n",
    "word_correction_dict(('Shoppee','ช็อปปี้','ชอปปี้','shopee','Shoppee','ชอบปี้','shoppee','ช้อปปิ้','platform'),'ช้อปปี้')\n",
    "word_correction_dict(('coin','คอยส์','coins','coincashback'),'คอยน์')\n",
    "word_correction_dict('จนท','เจ้าหน้าที่')\n",
    "word_correction_dict(('แอร์เพ','airpay','แอร์เพย์','Airpays','Airpay','Air pay','air pay','air pays','Airplay','airplay','แอเพลย์','แอร์เพลย์','AirPay'),'แอร์เพย์')\n",
    "word_correction_dict('ตำ','สั่ง')\n",
    "word_correction_dict('แกง','หลอก')\n",
    "word_correction_dict('ดือ','ดี')\n",
    "word_correction_dict('มปร','ไม่เป็นไร')\n",
    "word_correction_dict('ค้าส่ง','ค่าส่ง')\n",
    "word_correction_dict('พน','พรุ่งนี้')\n",
    "word_correction_dict('คสอ','เครื่องสำอางค์')\n",
    "word_correction_dict('ลค','ลูกค้า')\n",
    "word_correction_dict(('เคอรี่','kerry','Kerry'),'เคอร์รี่')\n",
    "word_correction_dict(('Ninjavan','นินจา แวน','ninja van', 'Ninja Van','นินจา','ninja','Ninja','ninjavan'),'นินจาแวน')\n",
    "word_correction_dict(('Shope Express','Shopee express','ShopeeExpress','ช้อปปี้ express','ช้อปปี้ Express','ช้อปปี้เอกเพลส','ช้อปปี้เอ็กเพลส'),'ช้อปปี้เอ็กซ์เพรส')\n",
    "word_correction_dict(('Best Express','Best express','BestExpress','bestexpress','BEST EXPRESS'),'เบสเอกเพลส')\n",
    "word_correction_dict(('flash','Falsh'),'แฟลช')\n",
    "word_correction_dict(('DHL','dhl'),'ดีเอชแอล')\n",
    "word_correction_dict(('ปณ', 'thaipost'),'ไปษณีย์')\n",
    "word_correction_dict(('app'),'แอพ')\n",
    "word_correction_dict(('ม๊วก','มั๊ก','มั่ก'),'มาก')\n",
    "word_correction_dict(('random','ramdom','แรนด้อม','แรด้อม'),'สุ่ม')\n",
    "word_correction_dict(('WTF','wtf','วดฟ'),'ห่วย')\n",
    "word_correction_dict(('มั่ยดั้ย'),'ไม่ได้')\n",
    "word_correction_dict(('free shipping'),'ส่งฟรี')\n",
    "word_correction_dict(('cashback','Cashback'),'แคชแบค')\n",
    "word_correction_dict(('ขอบคุ'),'ขอบคุณ')\n",
    "word_correction_dict(('กทม'),'กรุงเทพ')\n",
    "word_correction_dict(('Security'),'ความปลอดภัย')\n",
    "word_correction_dict(('Live','live','LIVE'),'ไลฟ์')\n",
    "word_correction_dict(('Preorder','Pre Order'),'พรีออเดอร์')\n",
    "word_correction_dict(('เสิร์จ','search','เสิช','เสิ้จ','เสิร์ท'),'ค้นหา') \n",
    "word_correction_dict(('delivery','shipping'),'ขนส่ง')\n",
    "word_correction_dict(('ปสด'),'ประสาทแดก')\n",
    "word_correction_dict(('เคียด'),'เครียด')\n",
    "word_correction_dict(('โค้ดลับ','Code','code','โค๊ด','โค้ต','โค้ลลด'),'โค้ด')\n",
    "word_correction_dict(('เวป','เว็บ','web','เว็ป'),'เว็บไซต์') \n",
    "word_correction_dict(('email','เมลล์','mail','อีเมล','เมล์'),'เมล')\n",
    "word_correction_dict(('love','Love','รั้ก'),'รัก')\n",
    "word_correction_dict(('happy','แฮปปี้','Happy'),'มีความสุข')\n",
    "word_correction_dict(('link','Link','ลิ้ง','ลิ้งปุ้'),'ลิ้งค์')\n",
    "word_correction_dict(('วอลเลท','wallet','วอเลต','wallet'),'วอลเล็ต')\n",
    "\n",
    "def preprocessor(sentence):\n",
    "    \n",
    "    #remove hashtag\n",
    "    hashtag_removed = re.sub(r\"#\\w+\",'', sentence) \n",
    "    \n",
    "    #remove url\n",
    "    url_removed = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','', hashtag_removed)\n",
    "\n",
    "    stop_words = list(pythainlp.corpus.thai_stopwords())\n",
    "    remove_words = ['นะค่ะ','นะคะ','ๆๆ','ๆ','ๆๆๆ','หรอ','งะ','เตง','วะ','อ่ะ','เออ','อะ','อ่า',\n",
    "              'นั้นแหละ','เหรอ','กก','ตะ','กะ','อะ','เปน','ที้ศื้อ','ก้','เดี๋ยวนี้','5000','500','11','39',\n",
    "               'ว่ะ','TvT','นัง','เด้อ','ก้อ','กุ','ค้าบ','90','การ','แทบจะ','โดยที่','10','200','เอาละ','special','อิ',\n",
    "               'เดือน','350','ฟุต','รอบ','รีด','169','150', '175', 'vogue','งี้', 'จก', 'จิงๆ','3000', '40', '4900', '50', \n",
    "               'clarins', 'morebeauties','แทบ','100', '300', 'flower','ซ้ำ','มุง','cpjgirlxx', 'คุณหนู','ฟี', 'มู้ด',\n",
    "               'ออ','199', 'cicishop','เดอะ', 'เบส','เดอะ', 'เบส','ทุ','แล้วจะ','ง่ะ','หก','เค้า','แห','ตั่ง','มค', 'ยัน',\n",
    "               'คา', 'ตอนนี้', 'ตะหาก','อะนะ', 'เย', 'แบ่บ','30', 'lover','ไหนจะ', 'case', 'ex','ยัย','สะ',\n",
    "               'คัด', 'ชิค', 'ฝอ','ววว','โล่', '399', 'pnjzyn', 'shoes', 'ก๊อกแก๊ก', 'คู่', 'จา','สนีก','อร', 'เก','99',\n",
    "               'มั้ง', 'เนี้ย','76', 'cotton','นา', 'ปัก','12','120','รต','รี่','เค','เพล', 'เอก', 'แก้', 'โช', '900',\n",
    "               'ซิ','เด', '69', 'กระจูด','Lock', 'amp','89', 'lemon', 'vibes','นะค้า','Roushun', 'Some', 'by', 'mi',\n",
    "               '800', 'ชี', 'นึง','490', 'divoom','ป่านนี้','ลาซาด้า','60', 'เธรด', '498','thread','everyday', 'look', \n",
    "               'น๊า','299','180', '29','36', '53', 'cm','600', 'ml','จ่ะ','Xiaomi','ปะ','แอยย','15','539', 'Rose',\n",
    "               '41','air', 'deerma', 'humidifier', 'xiaomi','20','ดิ','1200', 'official','119','อีฉัน', '3700', 'about', \n",
    "               'airpods', 'vickyshop','553','แฮะ','655','159', '78','45', '24', 'Centellian','on', 'sale', 'top',\n",
    "               '250', 'bentoy','vibe','Lazada','อี','booster', 'clear', 'nct', 'sm', 'sp','149', 'Plantnery', 'tea', \n",
    "               'tree', 'ๆๆ','CHEF', 'DREAM','grafen', 'yvescosmetic','503', '650', 'Klinn', 'Oxygen', 'diffuser', 'room',\n",
    "               'ฮะ','CJ', 'HYA', 'Vit', 'supermarket','dashing', 'diva', 'fit', 'slim', 'super','ทันใด','แหม', 'โห้ๆๆ', 'โอ',\n",
    "               'ๆผ', 'partner','พรีออเดอร์', '01', '08', '09', '22', '588', '699','gt','000', 'unpack',\n",
    "               'wfh','david', 'flash', 'jones','koreatownofficial','earbud', 'soundpeats', 'trueair','งับ','ค่าเธรด',\n",
    "               '573', '95', 'HJV','ง่า','25', 'Bears', 'Care','25', 'ขายของ', 'Bears', 'Care','ศศ','ลซด','Davidjones', \n",
    "               'แน่ๆ','294', '49', 'หมอน', 'bulky','ล่ะ', 'อ๊าว','tokpokki','449','46', 'start','Allie', 'Official', 'lt',\n",
    "               '990', 'Bud', 'Pro', 'Samsung', 'Store', 'Website','27','ยยยย','beauty', 'multy', '167', '23',\n",
    "               'Facebook', 'Morglorythailand', 'ORI', 'Page','2500','Mobile', 'WPN', 'mbk','55555', '2700', '880',\n",
    "               'Argan', 'Elegance', 'Hair', 'Oil', 'เอย', 'Select', 'Shop', 'Bulky','APEX', 'KOREA', 'konvy',\n",
    "               'account', 'sappun', 'cj', 'logistics', 'เอิ่ม', 'เอ่ย', '101', 'Cezanne', 'best', 'the', '34',\n",
    "               'Normal', 'Standard', '26','หรอก', '55', '74', 'qqaukbj','mini', 'projector','BULKY','172'\n",
    "               '400', 'FBSH','เอ้ย', 'เด่', 'เรย', 'POS','อีฟ', 'Bra', 'Jutui', '197', 'NATPATTY', '674','dm',\n",
    "               '15000', '318', 'Li', 'Net', 'Power', 'RPP', 'Remax', 'Type', 'Weight', 'bank', 'battery', \n",
    "               'iPhone', 'mAh', 'polymer', '850', '160', 'jt','555555555', '827','569', '70', '770', '790', 'Border',\n",
    "               'Day', 'Dm', 'One', 'POSTER', 'DM', 'Pt','5555555', 'คิคิ', '02', '21', 'nacific', 'out', 'sold',\n",
    "               '290', '320', 'Jackson', 'โอ้โห', '195', '390', 'CK', '35', '450', 'City', 'Marvelous', 'Size', 'fanmade',\n",
    "               'ๆๆไๆไ', 'วววว', 'ไต', '165', 'ๆๆๆๆๆ', '1700', 'hitachi', 'sharp', '00', '04', 'fee', 'ลทฟอม', 'buy', 'power',\n",
    "               'Hive', 'Rhythm', '630','77', '79', '394', 'KUNYA', '14', '18', 'Best', 'อุ', 'อ๋อ', '37', 'Stock', 'bio'\n",
    "               ,' ', 'TT', '4000', '196','ก', '140', '189', 'Later', 'Next', 'DairyHome', 'Inter', '925', '2020', '380', 'FIGARO', \n",
    "               'Japan', 'Madame', 'Nendoroid', 'วาก', 'BSWBD', 'readlenlen','Etude', '690', '188', 'flip', 'วอ', 'หว่า',\n",
    "               'ฮ', '214', 'daily', 'prize', '4.4','0.1', '0.2', '0.5','1300', '555555', 'pro', 'ultra', '1450', '55555555', 'chanel',\n",
    "               '7732', '135','netflix','ไอจี','ig','นี้แหละ','แล้วจะ','ที่จะ','วว', 'หวีด','Market', 'place','ไหม',\n",
    "               'เลย','สักวัน','พับจี','เล๊ย', 'พุ่ง','โด้', 'ดึง','ๆๆๆๆ','platinum','joox', 'rov', 'spotify','109','เซฟอล่า','325',\n",
    "               'cargo', 'tag', 'taobao', 'ป่ะ', 'อีกแล้ว','เนเจอร์','แมสก์','อยู่แล้ว', 'อิอิ','เตง','แล้วด้วย','TH','อม','22','โคตร',\n",
    "                'วว', 'อาโป','ตปท','แพลตินัม','ค่าบริการ','พับจี','เล๊ย','อีกครั้ง','sms','มามามู','อันนี้', 'เมื่อกี้','หน็อย','ไลน์', '17', 'กี', 'ค้า', 'ทท', \n",
    "                'นนน','หละ','อิ','งั้น','งั้น', 'สัก','แหม่','สส','แพทตินั่ม','Delivery', 'Same','พี่เฟย์', 'ปป', 'drop', 'mic','Link','ดดดด',\n",
    "                'โลด','order','แล้วก็','IG', 'save', 'อีกที','เอะอะ','lazada', 'closed','2021','บบ', 'CP','โก', 'ก้า', 'มูส','ถามติง','จิง',\n",
    "                'เห้ย', 'redeem','เดย์', 'ชม.', 'แป๊ปๆ','อยู่เลย', 'Apple','อิน', 'เจ้','นัท', '5555','ฮาวทู','นน', 'อืม', 'ว่าซั่น', 'เนี่ยะ','แล้วไป',\n",
    "                'มัวแต่', 'เซน', 'เต้อ', 'เวน', 'ดัน','สิ','สู', 'ซิลเวอร์', 'นัม','แพลตินัม','โถ่','ว่าแต่','เนอะ','อืน', 'อ่อ','ก๊อ','ยยย', 'professional',\n",
    "                'whoami', 'กุแหละ', '555','เหอะ', 'Platinum', 'Ac', 'Nattamon', 'natt','SKYWORTH', 'skyworth', 'คลิ้ก','gold', 'member','ฮู้', \n",
    "                '30000', 'Gold','ร๊ยย', 'วน', 'availability','1100', '1900','อิเกีย','48', 'รป', 'เห้อ', 'หึ','day', 'off','อีกด้วย','ไบร์ทมิวซ์','ยย',\n",
    "                'ดรีม','เวย์วี','เดะ', 'เด่ะ','โอ้ย', '11000','ติ้กตอก', 'ต่ะ','ป๋ม','โว้ย', 'wdm','52', '54','อยุ่ต้ะ','พก', 'ฟิวส์', 'ก้าด', 'พาหุรัด', 'สยาม', 'เฟรม', \n",
    "                'เวิร์ล', 'ไชน่า', 'said', 'เซ้น','ซัพน้องอี้ป๋อ','มุ้ยคั้บ', 'pika', 'มาร์ค','มั่ง','warota', 'ฟตซ','ว้ะ', 'เต๊าะ','candylab',\n",
    "                'มมม','น้ออ','dhl','เฮ้ย','ดด', 'มะ', 'วาน', '65','อาทิต','ป้ะ','มึ้งง', 'ย่ะ', '64', 'เอ่อ', '127', '205', 'อิบอพ','104', 'Card', 'Cashbee', \n",
    "                'น๊อ','อี๊', 'ems','EMS','SuperM','dvd', '115', 'เงี้ย','ลทบ', 'ฟตบ', 'วุย', 'เอ้อ', '19', 'ได่','no' ,'เอ๊ย','เอ้า','expless', 'นนนน',\n",
    "                '28', 'อ้าว', 'เซิร์ช', 'กค', 'คร๊', 'ร๊', 'ากกกกก', '', '80', 'ป๊ะ', 'ถถถ','หว่ส' ,'gif', 'jd','ฯ','ไม๊', 'Act', 'Again', 'Album', 'Dance', \n",
    "                'Gonna', 'Never', 'TAEMIN', 'The', 'rd', 'พิน', '탬니', 'studio', 'คั้บ', 'เน่อ', 'เบย', 'อ้ย', 'โป้', 'โฮป', 'chapter', 'mbookstoreshop', 'one'\n",
    "                , '42', 'io', 'แหะ', '88', 'นว', 'เน้าะ', 'ซอก', 'อู',' 85.', 'wabelle', '13','chocolate', 'ice', 'pack', '75', 'เนี่น','votive',\n",
    "                '59','JT','makeup', 'please', '750', 'มมมม', '47', 'adidas', 'kratop', 'Drop', 'Off','ก้', 'แน้ว', 'PVC', 'ล้ะ', '385', 'STD', '16',\n",
    "                 'Answer', 'is', 'unacceptable','WFH','1414', '1500', '1530', '170', '72','225', 'return', '130', 'มั๋ย', '2564', '110', '190', 'นนนนนน'\n",
    "                 , 'Xpres', 'shein', 'Gateway', 'ยยยยย', 'update', 'Only', 'at', 'ARM', 'FRP', 'FSTRMAR', 'Discount', 'Max', 'Min', 'WCSPORT',\n",
    "                  'Jamclub', 'Jamshop', 'Sale','มังงะ', 'มังฮวา', 'ว้อย', 'qr', 'ยยยยยย', 'Fabricposter', 'timeline', 'ช้อง', 'ยยยยยยยยย' , 'diy', 'house',\n",
    "                 'อ้ะ', 'Application','จวพ','56', 'ว้อททท', 'พค', 'ง.','.', 'เอ๊ะ', 'pt', '950', 'mandalaki', '1600', '1651', '400', '51', 'Classic', 'Silver',\n",
    "                  'กะรัต', '270', 'sliver', 'LAGOM','spay','อฟซ','me', 'remind','เนาะ','700','กก','SMASOCIAL', 'mall', 'ญญ', 'ฟพด', 'มุ', 'แมน', '31', 'ด์', \n",
    "                  'innisfree', 'ฟฟ','07','179', 'FASH', 'Season', 'TR', 'sweater', 'Rewards', 'น้องเบ', 'Ig', 'doubletttt', '️️', 'RE', 'Freesize', 'Tiny',\n",
    "                  '155','฿','FASH2TR','199', 'ANGELS', 'FENNEC', 'STRETCH', 'follow', 'บัก', 'อักษร', 'downy', 'tinytan', 'man', 'peter', 'unleashia', 'ทิ้นท์',\n",
    "                  'finnomena','จจจ', 'shopaholic', 'จว.', '405', '499', 'Wetv', 'expo', 'gadget', 'smartphone', 'งม', '224', 'ABIDE', '277', 'NADIA',\n",
    "                  'name','cf', 'Value', 'Dtac', 'Clarins', 'Galaxy', 'Smart', 'TV', 'sephora', 'เล', '', 'ecommerce','NCT', 'งื้ออ', 'mizumi',\n",
    "                   'ดดดดด','ลาซาด้า','น้องดรีม','อซท','applesheep', 'ยยยย้', 'ขข', 'เหม่', '590', '495', 'Watsons','550', '106', 'zoflora', 'Shopback',\n",
    "                  'ห้ะ','Brand','AirPods','2000','minimall','usb','xGOT', 'โบ๊ะบ๊ะ', '172','butterscotch', 'mart','เปน', 'ak', 'annywawony', 'line', 'outlet', 'th'\n",
    "                  'youus', '2790','Nintendo', 'Switch','44','BNK', 'Photoset', 'Up', 'scale','เห้ยย', '304', '419', 'CC', 'PA', 'SPF', 'Sunscreen', 'Tinted', 'Zunshield',\n",
    "                  'AA', 'Line', 'website', 'LZD', 'ssktmmee', 'sticker', 'tiny', '˃̵͈̑ᴗ˂̵͈̑', 'Airpods', '29','295', 'Ems', 'แจบ', 'acc', 'schwnn','Oppo', 'Reno', '1000', '85'\n",
    "                  , 'FB','1340', 'FOUNDATION', 'LONG', 'SKIN', 'WEAR', 'WEIGHTLESS', 'bobbi', 'brown', '2022', '202', 'Heroine', 'make', 'mascara', 'speedy',\n",
    "                  '279','ALBUM', 'ATEEZ', 'FEVER', 'MINI', 'ORDER', 'PRE', 'Part', 'ZERO', 'ver', 'boxbox', 'Oxe', 'acne', 'cure', 'lotion', 'powder',\n",
    "                  'Balm', 'HOrME', 'Relaxing', 'aroma','china', 'in', 'made', 'lilybyredthailand', 'blu', 'ray','Two', 'two','Bio','Momsta', 'dicon','beyondsoho',\n",
    "                  '126', '2017', 'riverside','38', 'manyo', 'low', 'stock', '230', '345','นิ้เบย','134', '206', 'Post', 'it', 'toxic', 'Marhen', 'เบยยย',\n",
    "                  'index', '1490', 'remax', 'apple', 'music', 'MBK', 'sony', 'Dairy', 'Home','insalon','naimx','Marimekko','ipad','178','91','silver',\n",
    "                  'everY', 'eloop','Lyrics', 'BB', 'MT', 'YG', '1690', 'hi', 'jet', 'melon', 'given', 'electrolux', 'kg', 'BOL', 'synnara','loft','face',\n",
    "                  '730', 'Arrival','โยชิ','811','lomo', 'หว่ะ', '108', 'viu', 'wetv','1360','xxx', 'Sulwhasoo','sos', 'isse', 'miyake','นน','facebook', 'twitter',\n",
    "                  'บร่ะ','ยยยยยย','ยยยย', 'kookshop', 'kr','nd', 'nude', 'rom', 'shell', '176', 'size','129', 'Shein', 'US', 'beautysite', 'international', 'norino', 'XL',\n",
    "                  '58','CE', '5555555555555', 'ads','ทะ', '1150', '3800','น', '217','207', 'roomieshobby','Fatal', 'Monsta','Holika', 'holika','qc',\n",
    "                  'Uki', 'stationery', 'rm', 'อุ๊ย', '43','ลาซาด้า', 'xl','ออฟฟิเชี่ยล','Member','ๆๆๆๆๆๆ','ok', '510', 'Midnight', 'AtrPay','ap',\n",
    "                  'refund','UX','762', 'if', 'then', 'banking','mamonde', 'Agoda', 'scb','220', '1042','222', '446', 'ก้บั่บ', 'Iam','BTS', 'google', 'แง่ง', 'โด้ย',\n",
    "                  'งื้อ','Scb','Update', 'later', 'electronics','2800','พิ้หมิวๆๆ','ป้ะ?', 'list', 'wish', 'BCURU', 'LVBUE', 'adapter', 'lightning', 'padair', 'type',\n",
    "                  'week','ยูทูป','แอนด์','จ๋วย']\n",
    "    for w in remove_words:\n",
    "        stop_words.append(w)\n",
    "\n",
    "    \n",
    "    screening_words = stop_words + remove_words\n",
    "    \n",
    "    merged = ''\n",
    "    #words = pythainlp.word_tokenize(str(sentence), engine='newmm')\n",
    "    # sentence = sentence.replace(':','')\n",
    "    words = set(thai_words()) # thai_words() returns frozenset คำที่ีใช้\n",
    "    tweet_words = ['ช้อปปี้','ช็อปปี้','แอร์เพ','ชอปปี้','แรนดอม','ทวิต','วอเลต','โดน','แอร์เพย์','เยอะ','คอยส์','คอยน์','ใช้','ไม่ได้',\n",
    "                  'ไอจี','สักที','เยอะ','แพนิค','Call Center','เอิร์ธโทน','ประทับใจ','แม่ค้า',\n",
    "                   'ยอดลดลง','เกาหลี','ส้นหนา','ยาวดี','จ้อจี้','บลูธูท','แพสชั่น','ติ๊กต๊อก','มินิมอล','สำอางค์','อัลบั้ม','ตีมือ'\n",
    "                   ,'ทิชชู่','เซรั่ม','ไฮยาลูรอน','ฟิน','มูฟออน','หนึบหนับ','คุมะมง','ออฟฟิเชียว', 'โจมาโลน', 'อิดอก', 'เอ็กเพลส',\n",
    "                   'อัลมอน', 'ปุ๊ปปั๊บ', 'แล้วจะ','โควท','หม้อทอดไร้น้ำมัน','อัติโนมัติ','เซนต์เปอร์','ประจำ','พนักงาน','แอดมิน','ติดต่อ','ธนาคาร',\n",
    "                   'ลาซาด้า','ผู้ถือหุ้น','สูญเสีย','ตกต่ำ','ค่าธรรมเนียม','รีทวิต','ติดต่อ','สิงคโปร์','ไทยแลนด์','เลื่อมล้ำ','โฟโต้การ์ด',\n",
    "                   'หาไม่เจอ','เจ้าหน้าที่','ช้อปปี้', 'แอดมิน', 'คอลเซ็นเตอร์','บริษัท', 'ค่าบริการ' ,'รับผิดชอบ', 'ติดต่อ' ,'โกงเงิน', 'บัญชี', 'ดำเนินการ', 'แบ่งชนชั้น', \n",
    "                   'คุยกับพนักงาน', 'ติดต่อ', 'บริการ', 'แจ้ง', 'ระดับ', 'พรีเมียม', 'เมมเบอร์', 'แบน', 'เลิกใช้', 'สื่อสาร', 'ร้องเรียน', 'เลิกช้อป', 'เว็บไซต์', 'เบอร์', 'โทร', \n",
    "                   'เมล', 'สนใจ', 'เพิกเฉย', 'รอสาย', 'ตัดสาย', 'ด่วน', 'เมล', 'โทร', 'ศูนย์บริการ', 'ปัญหา', 'ชดเชย', 'องค์กร', 'เลิกสั่ง' \n",
    "                   'ประทับใจ', 'แนะนำ', 'ชี้แจง', 'นโยบาย', 'สมาชิก', 'การตลาด', 'โฆษณา', 'ลูกค้า', 'ผู้ใช้บริการ', 'แพลตฟอร์ม', 'โฆษณา',\n",
    "                   'ไม่ติดต่อ', 'ยกเลิก', 'รอ', 'ช้า', 'นาน', 'ชั่วโมง', 'ไม่ได้รับ', 'ไม่ได้สินค้า', 'ปัญหา', 'ผู้ส่ง', 'พนักงาน', 'ขนส่ง', 'รอนาน', \n",
    "                   'บริการ', 'ไม่โทรมา','ส่ง','ไม่มาส่ง','ไม่ส่ง', 'สุภาพ', 'บอกทาง', 'เชค','เช็ค','ตรวจ','ที่อยู่', 'ส่งช้า', 'ส่งเร็ว', 'ส่งไว', \n",
    "                   'โทรมา', 'ไม่โทร', 'ไม่รับ', 'เลือกขนส่ง', 'เลือกขนส่งไม่ได้', 'เลือกบริษัทขนส่ง', 'ได้ของ', 'ส่งของ', 'กำหนด', 'บริษัทขนส่ง', 'ค่าส่ง', \n",
    "                   'เก็บเงินปลายทาง', 'เก็บปลายทาง', 'เลขพัสดุ', 'ค่าส่ง', 'จ่าหน้า','โค้ด', 'โค๊ด', 'โค้ดลด', 'โค้ดช้อปปี้', 'กดไม่ทัน', \n",
    "                   'ใช้ไม่ทัน', 'ค่าส่ง', 'ลดราคา', 'ส่วนลด', 'โปรโมชั่น', 'คุ้มค่า' ,'ไม่คุ้ม', 'ถูก','โปร', 'โปรช้อปปี้', 'บอกต่อ', 'ใช้ไม่ได้', 'ใช้โค้ดไม่ได้', 'คุณภาพ', 'ใช้ได้', \n",
    "                   'ขั้นต่ำ', 'โค้ดส่งฟรี', 'ส่งฟรี', 'แกง', 'ลูกค้า', 'ใหม่', 'ฟรี', 'แถม', 'เต็ม', 'กด', 'ทัน', 'เที่ยงคืน', 'แฟลชเซลล์', 'เมมเบอร์', 'สิทธิ์', \n",
    "                   'คอยส์', 'แคมเปญ', 'เหรียญ', 'จ่ายบิล', 'ดีล', 'รดน้ำต้นไม้', 'คูปอง', 'กิจกรรม','ช้อป', 'ซื้อ', 'ซื้อมาจาก', 'ร้านในช้อปปี้', 'ไลฟ์', 'ไลฟ์สด', 'ในช้อปปี้', \n",
    "                   'จากช้อปปี้','สั่งช้อปปี้', 'เข้าช้อปปี้', 'ที่ช้อปปี้', 'ราคา', 'คุ้ม', 'คุ้มค่า', 'ไม่คุ้ม', 'แพง', 'ถูก', 'ลด', 'บอกต่อ', 'คุ้ม', 'คุณภาพ', \n",
    "                   'ซื้อ', 'ส่วนต่าง', 'ควรค่า','ควรซื้อ', 'ไปตำ', 'แนะนำ', 'ร้าน', 'ร้านค้า', 'พิกัด', 'ร้าน', 'รีวิว', 'สั่ง', 'สอย', 'ซื้อจาก', 'รวมของที่ซื้อ', 'สินค้า', \n",
    "                   'ของ', 'ขาย', 'ขายของ', 'บาท', 'ใน', 'จาก', 'พร้อมส่ง', 'คำสั่งซื้อ', 'รายงานผู้ใช้', 'แม่ค้า', 'พ่อค้า', 'โกง', 'ผู้ขาย', 'แกง', 'ร้านเสื้อผ้า', \n",
    "                   'เสียเงิน', 'ขายที่', 'ขายดี', 'หมดไว', 'ปลอม','แอพ', 'แอพช้อปปี้','แอพลิเคชั่น', 'หน้าจอ', 'รวน', 'สมัคร', 'ไถ', 'ไถช้อปปี้', 'ชั่วคราว', 'ระบบ', \n",
    "                   'เงิน', 'เว็บ', 'หน้าเว็บ', 'ตัด', 'บัตร','แคนเซิล', 'ยกเลิก', 'เป็นอะไร','พัง', 'ค้าง', 'ล่ม', 'เสีย', 'ไม่ได้', 'เด้ง', 'รอ', 'พัฒนา', \n",
    "                   'หักตังค์', 'หักเงิน', 'คืนเงิน', 'ตัดเงิน', 'ตัดบัตร', 'บัตร', 'หักเงิน', 'ยาก', 'ง่าย', 'พัฒนา', 'ไม่ตรง', 'โอน', 'โอนเงิน', 'เครดิต', \n",
    "                   'คืน', 'แก้', 'ด่วน', 'ขัดข้อง', 'ฟื้น', 'แก้ไข', 'จ่ายเงิน', 'ผ่อน', 'ปัญหา', 'โดนระงับ', 'ตรวจสอบ', 'ขั้นตอน', 'เงินไม่เข้า', 'เติมเงิน', 'ไม่เข้า', \n",
    "                   'จ่ายไม่ได้', 'แบบใหม่', 'แบบเก่า', 'บัตรเครดิต', 'บอท', 'อัพเดต', 'เงินคืน', 'บัญชี', 'อนุมัติ', 'กดเงิน', 'ช้อปปี้เพย์', 'แอร์เพย์', 'จอ', 'อัตโนมัติ', \n",
    "                   'ฟีด', 'โหมดดาร์ก', 'สถานะ', 'แฮ็ค', 'ตะกร้า', 'แจ้งเตือน', 'ระงับ', 'ยกเลิก', 'คำสั่งซื้อ', 'ตรวจสอบ', 'ลิ้งค์', 'สะดวก', 'ระบบล่ม', 'เติม', \n",
    "                   'โหลด', 'ปัดจอ', 'โหลด', 'เสิร์ช' ,'หมวด', 'ธุรกรรม', 'วงเงิน', 'วอลเล็ต','รีฟันด์','ท่องโลก','หลงไหล','เปอร์เซ็น','คนละครึ่ง','วอชเชอร์','บราวนี่',\n",
    "                   'กาแฟ่','เม้าส์','ออฟฟิต','เมนชั่น','โฟโต้ช้อป', 'เฟอร์นิเจอ','จิตใจ','ป้ายยา', 'ปุ๊กปิ๊ก','อินฟลูเซอร์','ระบุ','ขี้งก','ศีลเสมอกัน','เมมเบอร์','คลีนิก','ตาแตก'\n",
    "                    ,'เปียกน้ำ','โลภ','ดีเอชแอล','แฟลช','นินจาแวน','ช้อปปี้เอ็กซ์เพรส','เบสเอกเพลส','เคอร์รี่','ชำระ','ล่ำลือ','สีกรม','ไหนคะ','อิควัย','บัดซบ','แบนอยู่',\n",
    "                   'เปย์','แดรี่โฮม','หมั่นโหนก','กี๊ด','อัดเทป','ฉัน','เธอ','คุณ','เบื่อ', 'ตรุษจีน','เร็ว','ชิบหาย','ชิปหาย','มัดย้อม','โคย', 'แม็กเน็ต', 'อนิเมะ','มังงะ', \n",
    "                   'มังฮวา','สลีปปิ้งมาส์ก','ช้อปลาเนจ','มิสเพริกริน','ไดโซะ','เลิศ', 'ครัวซองค์','ออเดอร์','เดียว','ขนส่ง','แอค','สติ้กเกอร์','กลิ้ตเตอร์','รอยอลคันนิง',\n",
    "                   'เจลว่านหาง','เดรบิต','เหลือ','ช็อปแบค','จน','ราคา','กาตูน','อิโมจิ','บิสกิต','ทิ้ง','เสือก','มิว','ฮีล','อัลมอนต์','ซอส','ป้อบคอร์น','ออฟฟิเชี่ยล',\n",
    "                   'เซิ่นเจิ่น','วอลเลท','กรอง','ห่วยแตก','ห่วย','การันตี','สาระแน','น่ารักมาก','น่ารักจัง','น่ารักจริง','ลดเยอะดี','ไวมาก','มีความสุข','ไปจัด','ดีงาม',\n",
    "                    'ดีงามมาก','ปังจริง','สนใจ','ให้ความช่วยเหลือดีมาก','อิน','เยียวยาจิตใจ','อยากได้หมด','สุดยอด','แอดมินน่ารัก','ไม่โดนโกง','อิสัส','บอกลา','ซื้อด้วยไม่ไหว',\n",
    "                    'โดนหลอก','ทุเรศ','อิดอก','สัส','ช้อปปี้ก็โกง','อย่างเศร้า','เอาเปรียบ','สาระแน','ควย','พนักงานไม่สนใจ','เอาเปรียบ','เจ็บใจ','ตบกับกู','ไม่เข้ารับพัสดุ',\n",
    "                    'อิเวร','ตอแหล','อมเงิน','นังทรยศ','มิจฉาชีพ','โกงเงิน','เสียลูกค้า','ปรับปรุงแล้วแย่','แบน','ลบบริษัทช้อปปี้','พังพินาศ','โกรธบริษัทช้อปปี้','เหม็นหน้า',\n",
    "                    'ไม่ชอบแอดมิน','ชิปหาย','เลิกซื้อ','รีพอท','ห่วยแตก','ห่าเหว','เลิกใช้','บริการไม่ดี','พนักงานแย่','อิเหี้ย','โกง','เกลียดมัน','โคตรแย่','ลาก่อนบริษัทช้อปปี้',\n",
    "                    'เซงมาก','เลิกใช้บริษัทช้อปปี้','เลือกขนส่งไม่ได้','ค่าส่งแพง','งงมาก','อย่าหาทำ','โจร','ใจเกือบพัง','ห่วยแตก','ขนส่งห่วย','เสียความรู้สึก','ระบบสุ่มขนส่ง',\n",
    "                    'แพงจริง','เลื่อนส่งของ','สุ่มขนส่ง','เลือกขนส่ง','สาระแนเลือกขนส่ง','ประสาท','บรรลัย','ค่าส่งแพงเกิน','เลิกสั่ง','ไม่ส่งของ','ค่าส่งแรงมาก','ระบบห่วยแท้',\n",
    "                    'ขนส่งที่ได้แย่มาก','อยากจะด่า','พ่อมึงตาย','เลือกขนส่งเองก็ไม่ได้','สาระแนส่งของ','ห่วยมาก','งงมาก','เป็นเหี้ยไร','ยกเลิกออเดอร์','ตอแหลสถานะ','อิ','เหี้ย',\n",
    "                    'ขนส่งช้ามาก','ขนส่งช้า','อย่าสาระแน','เอาเปรียบ','บ้าบอคอแตก','ห่วยแตกมาก','นานชิบหาย','โคตรห่วย','เหี้ยจริง','อีควาย','ส่งฟรีที่ไม่มีอยู่จริง','อิควาย','ส่งช้ามาก',\n",
    "                    'เลือกขนส่งเองไม่ได้','ขนส่งห่วย','ขนส่งก็แย่','เปลี่ยนระบบขนส่ง','โคตรซวย','อิเวร','อิเวน','เกลียดช้อปปี้','เซ็ง','อิควัย','ไม่มีมารยาท','อยากสั่งของ','เลือกขนส่งไม่ได้',\n",
    "                    ]\n",
    "    for w in tweet_words:\n",
    "        words.add(w)\n",
    "    \n",
    "    #word tokenize\n",
    "    custom_tokenizer = Tokenizer(words)\n",
    "    custom_dictionary_trie = Trie(words)\n",
    "    words = pythainlp.word_tokenize(str(sentence), \n",
    "                                  custom_dict=custom_dictionary_trie, \n",
    "                                  engine='newmm')\n",
    "\n",
    "    for word in custom_tokenizer.word_tokenize(sentence):\n",
    "        word_norm = normalize(word)\n",
    "        if word_norm in correction_dict:\n",
    "            word_norm = correction_dict[word_norm]\n",
    "        if word_norm not in screening_words and word_norm!='' and len(word_norm)>1:\n",
    "            merged = merged + ',' + word_norm\n",
    "    return merged[1:]\n",
    "    #print('ชอปปี้' in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_text']\n",
    "y = df[['Topics', 'Sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "# clf = OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, y)\n",
    "# clf.predict(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: Company\n",
      "428 428\n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.73      0.69      0.71       134\n",
      "         neu       0.58      0.61      0.60       134\n",
      "         pos       0.63      0.63      0.63       134\n",
      "\n",
      "    accuracy                           0.65       402\n",
      "   macro avg       0.65      0.65      0.65       402\n",
      "weighted avg       0.65      0.65      0.65       402\n",
      "\n",
      "[[93 23 18]\n",
      " [21 82 31]\n",
      " [13 36 85]]\n",
      "\n",
      "\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.63      0.60      0.62        40\n",
      "         neu       0.50      0.47      0.48        34\n",
      "         pos       0.41      0.45      0.43        33\n",
      "\n",
      "    accuracy                           0.51       107\n",
      "   macro avg       0.51      0.51      0.51       107\n",
      "weighted avg       0.52      0.51      0.52       107\n",
      "\n",
      "[[24  6 10]\n",
      " [ 6 16 12]\n",
      " [ 8 10 15]]\n",
      "\n",
      "\n",
      "\n",
      "Topics: Logistics\n",
      "897 897\n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.68      0.72      0.70       158\n",
      "         neu       0.68      0.64      0.66       158\n",
      "         pos       0.73      0.73      0.73       158\n",
      "\n",
      "    accuracy                           0.70       474\n",
      "   macro avg       0.70      0.70      0.70       474\n",
      "weighted avg       0.70      0.70      0.70       474\n",
      "\n",
      "[[114  25  19]\n",
      " [ 33 101  24]\n",
      " [ 21  22 115]]\n",
      "\n",
      "\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.87      0.66      0.75       141\n",
      "         neu       0.34      0.55      0.42        44\n",
      "         pos       0.49      0.59      0.53        39\n",
      "\n",
      "    accuracy                           0.62       224\n",
      "   macro avg       0.57      0.60      0.57       224\n",
      "weighted avg       0.70      0.62      0.65       224\n",
      "\n",
      "[[93 34 14]\n",
      " [10 24 10]\n",
      " [ 4 12 23]]\n",
      "\n",
      "\n",
      "\n",
      "Topics: Promotion\n",
      "484 484\n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.63      0.70      0.66       130\n",
      "         neu       0.60      0.50      0.55       130\n",
      "         pos       0.69      0.73      0.71       130\n",
      "\n",
      "    accuracy                           0.64       390\n",
      "   macro avg       0.64      0.64      0.64       390\n",
      "weighted avg       0.64      0.64      0.64       390\n",
      "\n",
      "[[91 25 14]\n",
      " [36 65 29]\n",
      " [17 18 95]]\n",
      "\n",
      "\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.74      0.53      0.62        43\n",
      "         neu       0.48      0.47      0.47        45\n",
      "         pos       0.52      0.73      0.61        33\n",
      "\n",
      "    accuracy                           0.56       121\n",
      "   macro avg       0.58      0.58      0.57       121\n",
      "weighted avg       0.58      0.56      0.56       121\n",
      "\n",
      "[[23 16  4]\n",
      " [ 6 21 18]\n",
      " [ 2  7 24]]\n",
      "\n",
      "\n",
      "\n",
      "Topics: Store\n",
      "1256 1256\n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.57      0.59      0.58       347\n",
      "         neu       0.56      0.52      0.54       347\n",
      "         pos       0.57      0.60      0.58       347\n",
      "\n",
      "    accuracy                           0.57      1041\n",
      "   macro avg       0.57      0.57      0.57      1041\n",
      "weighted avg       0.57      0.57      0.57      1041\n",
      "\n",
      "[[204  74  69]\n",
      " [ 80 179  88]\n",
      " [ 75  65 207]]\n",
      "\n",
      "\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.44      0.51      0.47        87\n",
      "         neu       0.59      0.48      0.53       126\n",
      "         pos       0.41      0.44      0.42       102\n",
      "\n",
      "    accuracy                           0.48       315\n",
      "   macro avg       0.48      0.48      0.47       315\n",
      "weighted avg       0.49      0.48      0.48       315\n",
      "\n",
      "[[44 18 25]\n",
      " [25 61 40]\n",
      " [32 25 45]]\n",
      "\n",
      "\n",
      "\n",
      "Topics: System\n",
      "501 501\n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.64      0.71      0.68       110\n",
      "         neu       0.65      0.64      0.65       110\n",
      "         pos       0.73      0.67      0.70       110\n",
      "\n",
      "    accuracy                           0.67       330\n",
      "   macro avg       0.67      0.67      0.67       330\n",
      "weighted avg       0.67      0.67      0.67       330\n",
      "\n",
      "[[78 20 12]\n",
      " [24 70 16]\n",
      " [19 17 74]]\n",
      "\n",
      "\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.55      0.51      0.53        57\n",
      "         neu       0.41      0.39      0.40        41\n",
      "         pos       0.33      0.41      0.37        27\n",
      "\n",
      "    accuracy                           0.45       125\n",
      "   macro avg       0.43      0.44      0.43       125\n",
      "weighted avg       0.46      0.45      0.45       125\n",
      "\n",
      "[[29 13 15]\n",
      " [18 16  7]\n",
      " [ 6 10 11]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models_raw = {}\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "for topics in ['Company','Logistics','Promotion','Store','System']:\n",
    "    \n",
    "    print('Topics:', topics)\n",
    "    \n",
    "    \n",
    "    pipeline_raw = Pipeline([\n",
    "        ('bow', CountVectorizer(analyzer=preprocessor)),  # strings to token integer counts\n",
    "        ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "        ('classifier',OneVsOneClassifier(LinearSVC()))])  # train on TF-IDF vectors \n",
    "    \n",
    "    X_train_raw = X_train[y_train['Topics'] == topics]\n",
    "    y_train_raw = y_train['Sentiment'][y_train['Topics'] == topics]\n",
    "    print(len(X_train_raw), len(y_train_raw))\n",
    "    \n",
    "    vectorizer_vec = CountVectorizer(analyzer=preprocessor)\n",
    "    X_train_raw_vec = vectorizer_vec.fit_transform(X_train_raw)\n",
    "    \n",
    "    vectorizer_tf = TfidfTransformer()\n",
    "    X_train_raw_tf = vectorizer_tf.fit_transform(X_train_raw_vec)\n",
    "    \n",
    "    #Undersampling\n",
    "    X_train_raw_un, y_train_raw_un = rus.fit_sample(X_train_raw_tf, y_train_raw)\n",
    "#   print(len(X_train_raw_up), len(y_train_raw_up))\n",
    "    \n",
    "    model_raw = LinearSVC()\n",
    "    model_raw.fit(X_train_raw_un, y_train_raw_unn)\n",
    "    \n",
    "    \n",
    "#   pipeline_raw.fit(X_train_raw_up, y_train_raw_up)\n",
    "    models_raw[topics] = model_raw\n",
    "    \n",
    "    \n",
    "    X_test_raw_vec = vectorizer_vec.transform(X_test[y_test['Topics'] == topics])\n",
    "    X_test_raw_tf = vectorizer_tf.transform(X_test_raw_vec)\n",
    "    \n",
    "    y_predict_raw_train = model_raw.predict(X_train_raw_un)\n",
    "    y_predict_raw = model_raw.predict(X_test_raw_tf)\n",
    "    \n",
    "    print('train')\n",
    "    print(classification_report(y_train_raw_up,y_predict_raw_train))\n",
    "    print(confusion_matrix(y_train_raw_up,y_predict_raw_train))\n",
    "    print('\\n'*2)\n",
    "\n",
    "    print('test')\n",
    "    print(classification_report(y_test['Sentiment'][y_test['Topics'] == topics],y_predict_raw))\n",
    "    print(confusion_matrix(y_test['Sentiment'][y_test['Topics'] == topics],y_predict_raw))\n",
    "    print('\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: Company\n",
      "428 428\n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.73      0.70      0.72       159\n",
      "         neu       0.62      0.64      0.63       159\n",
      "         pos       0.67      0.67      0.67       159\n",
      "\n",
      "    accuracy                           0.67       477\n",
      "   macro avg       0.67      0.67      0.67       477\n",
      "weighted avg       0.67      0.67      0.67       477\n",
      "\n",
      "[[112  27  20]\n",
      " [ 25 102  32]\n",
      " [ 17  36 106]]\n",
      "\n",
      "\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.63      0.65      0.64        40\n",
      "         neu       0.53      0.47      0.50        34\n",
      "         pos       0.36      0.39      0.38        33\n",
      "\n",
      "    accuracy                           0.51       107\n",
      "   macro avg       0.51      0.50      0.51       107\n",
      "weighted avg       0.52      0.51      0.52       107\n",
      "\n",
      "[[26  3 11]\n",
      " [ 6 16 12]\n",
      " [ 9 11 13]]\n",
      "\n",
      "\n",
      "\n",
      "Topics: Logistics\n",
      "897 897\n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.70      0.66      0.68       564\n",
      "         neu       0.70      0.70      0.70       564\n",
      "         pos       0.75      0.79      0.77       564\n",
      "\n",
      "    accuracy                           0.72      1692\n",
      "   macro avg       0.71      0.72      0.71      1692\n",
      "weighted avg       0.71      0.72      0.71      1692\n",
      "\n",
      "[[372 113  79]\n",
      " [ 99 393  72]\n",
      " [ 60  58 446]]\n",
      "\n",
      "\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.88      0.67      0.76       141\n",
      "         neu       0.31      0.45      0.37        44\n",
      "         pos       0.48      0.64      0.55        39\n",
      "\n",
      "    accuracy                           0.62       224\n",
      "   macro avg       0.56      0.59      0.56       224\n",
      "weighted avg       0.70      0.62      0.65       224\n",
      "\n",
      "[[95 33 13]\n",
      " [10 20 14]\n",
      " [ 3 11 25]]\n",
      "\n",
      "\n",
      "\n",
      "Topics: Promotion\n",
      "484 484\n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.67      0.66      0.66       181\n",
      "         neu       0.62      0.54      0.58       181\n",
      "         pos       0.67      0.76      0.71       181\n",
      "\n",
      "    accuracy                           0.65       543\n",
      "   macro avg       0.65      0.65      0.65       543\n",
      "weighted avg       0.65      0.65      0.65       543\n",
      "\n",
      "[[120  36  25]\n",
      " [ 39  98  44]\n",
      " [ 21  23 137]]\n",
      "\n",
      "\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.67      0.51      0.58        43\n",
      "         neu       0.52      0.51      0.52        45\n",
      "         pos       0.52      0.70      0.60        33\n",
      "\n",
      "    accuracy                           0.56       121\n",
      "   macro avg       0.57      0.57      0.56       121\n",
      "weighted avg       0.57      0.56      0.56       121\n",
      "\n",
      "[[22 16  5]\n",
      " [ 6 23 16]\n",
      " [ 5  5 23]]\n",
      "\n",
      "\n",
      "\n",
      "Topics: Store\n",
      "1256 1256\n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.58      0.63      0.60       502\n",
      "         neu       0.57      0.50      0.53       502\n",
      "         pos       0.57      0.58      0.57       502\n",
      "\n",
      "    accuracy                           0.57      1506\n",
      "   macro avg       0.57      0.57      0.57      1506\n",
      "weighted avg       0.57      0.57      0.57      1506\n",
      "\n",
      "[[316  95  91]\n",
      " [117 252 133]\n",
      " [115  95 292]]\n",
      "\n",
      "\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.44      0.54      0.49        87\n",
      "         neu       0.58      0.48      0.52       126\n",
      "         pos       0.42      0.43      0.43       102\n",
      "\n",
      "    accuracy                           0.48       315\n",
      "   macro avg       0.48      0.48      0.48       315\n",
      "weighted avg       0.49      0.48      0.48       315\n",
      "\n",
      "[[47 16 24]\n",
      " [29 60 37]\n",
      " [30 28 44]]\n",
      "\n",
      "\n",
      "\n",
      "Topics: System\n",
      "501 501\n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.67      0.71      0.69       227\n",
      "         neu       0.68      0.67      0.67       227\n",
      "         pos       0.76      0.74      0.75       227\n",
      "\n",
      "    accuracy                           0.70       681\n",
      "   macro avg       0.71      0.70      0.71       681\n",
      "weighted avg       0.71      0.70      0.71       681\n",
      "\n",
      "[[161  40  26]\n",
      " [ 50 151  26]\n",
      " [ 29  30 168]]\n",
      "\n",
      "\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.59      0.53      0.56        57\n",
      "         neu       0.44      0.39      0.42        41\n",
      "         pos       0.32      0.44      0.37        27\n",
      "\n",
      "    accuracy                           0.46       125\n",
      "   macro avg       0.45      0.45      0.45       125\n",
      "weighted avg       0.48      0.46      0.47       125\n",
      "\n",
      "[[30 12 15]\n",
      " [14 16 11]\n",
      " [ 7  8 12]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models_raw = {}\n",
    "smote = SMOTE()\n",
    "\n",
    "for topics in ['Company','Logistics','Promotion','Store','System']:\n",
    "    \n",
    "    print('Topics:', topics)\n",
    "    \n",
    "    \n",
    "    pipeline_raw = Pipeline([\n",
    "        ('bow', CountVectorizer(analyzer=preprocessor)),  # strings to token integer counts\n",
    "        ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "        ('classifier',OneVsOneClassifier(LinearSVC()))])  # train on TF-IDF vectors \n",
    "    \n",
    "    X_train_raw = X_train[y_train['Topics'] == topics]\n",
    "    y_train_raw = y_train['Sentiment'][y_train['Topics'] == topics]\n",
    "    print(len(X_train_raw), len(y_train_raw))\n",
    "    \n",
    "    vectorizer_vec = CountVectorizer(analyzer=preprocessor)\n",
    "    X_train_raw_vec = vectorizer_vec.fit_transform(X_train_raw)\n",
    "    \n",
    "    vectorizer_tf = TfidfTransformer()\n",
    "    X_train_raw_tf = vectorizer_tf.fit_transform(X_train_raw_vec)\n",
    "    \n",
    "    #Upsampling\n",
    "    X_train_raw_up, y_train_raw_up = smote.fit_sample(X_train_raw_tf, y_train_raw)\n",
    "#     print(len(X_train_raw_up), len(y_train_raw_up))\n",
    "    \n",
    "    model_raw = LinearSVC()\n",
    "    model_raw.fit(X_train_raw_up, y_train_raw_up)\n",
    "    \n",
    "    \n",
    "#   pipeline_raw.fit(X_train_raw_up, y_train_raw_up)\n",
    "    models_raw[topics] = model_raw\n",
    "    \n",
    "    \n",
    "    X_test_raw_vec = vectorizer_vec.transform(X_test[y_test['Topics'] == topics])\n",
    "    X_test_raw_tf = vectorizer_tf.transform(X_test_raw_vec)\n",
    "    \n",
    "    y_predict_raw_train = model_raw.predict(X_train_raw_up)\n",
    "    y_predict_raw = model_raw.predict(X_test_raw_tf)\n",
    "    \n",
    "    print('train')\n",
    "    print(classification_report(y_train_raw_up,y_predict_raw_train))\n",
    "    print(confusion_matrix(y_train_raw_up,y_predict_raw_train))\n",
    "    print('\\n'*2)\n",
    "\n",
    "    print('test')\n",
    "    print(classification_report(y_test['Sentiment'][y_test['Topics'] == topics],y_predict_raw))\n",
    "    print(confusion_matrix(y_test['Sentiment'][y_test['Topics'] == topics],y_predict_raw))\n",
    "    print('\\n'*2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
