{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "import pythainlp\n",
    "from pythainlp.corpus.common import thai_words\n",
    "from pythainlp.tokenize import word_tokenize, Tokenizer\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "#import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pythainlp.util.trie import Trie\n",
    "from pythainlp.util import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pythainlp.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"./Shopee_Tweet_NEW.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Create_at</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>ใช่ค่ะ ตอนแรกบอกว่าให้กดรับของได้เลยแล้วจะส่งใ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>อยากให้เรื่องนี้แมส เผื่อว่า จะออกมารับผิดชอบอ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-06</td>\n",
       "      <td>ขอบคุณบริษัทช้อปปี้มากฮะที่แอดมินพาพี่มายกับอา...</td>\n",
       "      <td>pos</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-26</td>\n",
       "      <td>บอกลาบริษัทช้อปปี้ก่อนนะ แอดมินแบบนี้ ซื้อด้วย...</td>\n",
       "      <td>neg</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>เรากดสั่งของไปรอของมาจนวันนี้ ไม่ส่งซักที วันน...</td>\n",
       "      <td>neg</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Create_at                                         clean_text Sentiment  \\\n",
       "0 2021-01-05  ใช่ค่ะ ตอนแรกบอกว่าให้กดรับของได้เลยแล้วจะส่งใ...       neg   \n",
       "1 2021-01-05  อยากให้เรื่องนี้แมส เผื่อว่า จะออกมารับผิดชอบอ...       neg   \n",
       "2 2021-02-06  ขอบคุณบริษัทช้อปปี้มากฮะที่แอดมินพาพี่มายกับอา...       pos   \n",
       "3 2021-03-26  บอกลาบริษัทช้อปปี้ก่อนนะ แอดมินแบบนี้ ซื้อด้วย...       neg   \n",
       "4 2021-02-01  เรากดสั่งของไปรอของมาจนวันนี้ ไม่ส่งซักที วันน...       neg   \n",
       "\n",
       "    Topics  \n",
       "0  Company  \n",
       "1  Company  \n",
       "2  Company  \n",
       "3  Company  \n",
       "4  Company  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Create_at\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_text    0\n",
       "Sentiment     0\n",
       "Topics        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add word in dict\n",
    "custom_dict = set(thai_words())\n",
    "\n",
    "Company_ = 'บริษัทช้อปปี้ แอดมิน คอลเซ็นเตอร์ เจ้าหน้าที่ พนักงาน บริษัท ค่าบริการ รับผิดชอบ ติดต่อ โกงเงิน บัญชี ดำเนินการ แบ่งชนชั้น คุยกับพนักงาน ติดต่อ บริการ แจ้ง ระดับ พรีเมียม เมมเบอร์ แบน เลิกใช้ สื่อสาร ร้องเรียน เลิกช้อป เว็บไซต์ เบอร์ โทร เมล สนใจ เพิกเฉย รอสาย ตัดสาย ด่วน เมล โทร ศูนย์บริการ ปัญหา ชดเชย องค์กร เลิกสั่ง ประทับใจ แนะนำ ชี้แจง นโยบาย สมาชิก การตลาด โฆษณา ลูกค้า ผู้ใช้บริการ แพลตฟอร์ม โฆษณา'\n",
    "Company_Topics = Company_.split(' ')\n",
    "\n",
    "Logistics_ = 'ไม่ติดต่อ ยกเลิก รอ ช้า นาน ชั่วโมง ไม่ได้รับ ไม่ได้สินค้า ปัญหา ผู้ส่ง พนักงาน ขนส่ง รอนาน บริการ ไม่โทรมา ส่ง ไม่มาส่ง ไม่ส่ง สุภาพ บอกทาง เชค เช็ค ตรวจ ที่อยู่ ส่งช้า ส่งเร็ว ส่งไว โทรมา ไม่โทร ไม่รับ เลือกขนส่ง เลือกขนส่งไม่ได้ เลือกบริษัทขนส่ง ได้ของ ส่งของ กำหนด บริษัทขนส่ง ค่าส่ง เก็บเงินปลายทาง เก็บปลายทาง เลขพัสดุ ค่าส่ง จ่าหน้า'\n",
    "Logistics_Topics = Logistics_.split(' ')\n",
    "\n",
    "Promotion_ =  'โค้ด โค๊ด โค้ดลด โค้ดช้อปปี้ กดไม่ทัน ใช้ไม่ทัน ค่าส่ง ลดราคา ส่วนลด โปรโมชั่น คุ้มค่า ไม่คุ้ม ถูก โปร โปรช้อปปี้ บอกต่อ ใช้ไม่ได้ ใช้โค้ดไม่ได้ คุณภาพ ใช้ได้ ขั้นต่ำ โค้ดส่งฟรี ส่งฟรี แกง ลูกค้า ใหม่ ฟรี แถม เต็ม กด ทัน เที่ยงคืน แฟลชเซลล์ เมมเบอร์ สิทธิ์ คอย คอยส์ แคมเปญ เหรียญ จ่ายบิล ดีล รดน้ำต้นไม้ คูปอง กิจกรรม'\n",
    "Promotion_Topics = Promotion_.split(' ')\n",
    "\n",
    "Store_ =  'ช้อป ซื้อ ซื้อมาจาก ร้านในช้อปปี้ ไลฟ์ ไลฟ์สด ในช้อปปี้ จากช้อปปี้ สั่งช้อปปี้ เข้าช้อปปี้ ที่ช้อปปี้ ราคา คุ้ม คุ้มค่า ไม่คุ้ม แพง ถูก ลด บอกต่อ คุ้ม คุณภาพ ซื้อ ส่วนต่าง ควรค่า ควรซื้อ ไปตำ แนะนำ ร้าน ร้านค้า พิกัด ร้าน รีวิว สั่ง สอย ซื้อจาก รวมของที่ซื้อ สินค้า ของ ขาย ขายของ บาท ใน จาก พร้อมส่ง คำสั่งซื้อ รายงานผู้ใช้ แม่ค้า พ่อค้า โกง ผู้ขาย แกง ร้านเสื้อผ้า เสียเงิน ขายที่ ขายดี หมดไว ปลอม'\n",
    "Store_Topics = Store_.split(' ')\n",
    "\n",
    "System_ = 'แอพ แอพช้อปปี้ แอพลิเคชั่น หน้าจอ รวน สมัคร ไถ ไถช้อปปี้ ชั่วคราว ระบบ เงิน เว็บ หน้าเว็บ ตัด บัตร แคนเซิล ยกเลิก เป็นอะไร พัง ค้าง ล่ม เสีย ไม่ได้ เด้ง รอ พัฒนา หักตังค์ หักเงิน คืนเงิน ตัดเงิน ตัดบัตร บัตร หักเงิน ใช้ ยาก ง่าย พัฒนา ไม่ตรง โอน โอนเงิน เครดิต คืน แก้ ด่วน ขัดข้อง ฟื้น แก้ไข จ่ายเงิน ผ่อน ปัญหา โดนระงับ ตรวจสอบ ขั้นตอน เงินไม่เข้า เติมเงิน ไม่เข้า จ่ายไม่ได้ แบบใหม่ แบบเก่า บัตรเครดิต บอท อัพเดต เงินคืน บัญชี อนุมัติ กดเงิน ช้อปปี้เพย์ แอร์เพย์ จอ อัตโนมัติ ฟีด โหมดดาร์ก สถานะ แฮ็ค ตะกร้า แจ้งเตือน ระงับ ยกเลิก คำสั่งซื้อ ตรวจสอบ ลิ้งค์ สะดวก ระบบล่ม เติม โหลด ปัดจอ โหลด เสิร์ช หมวด ธรรมเนียม ธุรกรรม วงเงิน วอลเล็ต'\n",
    "System_Topics = System_.split(' ')\n",
    "\n",
    "reviews_dict = Company_Topics+Logistics_Topics+Promotion_Topics+Store_Topics+System_Topics\n",
    "\n",
    "for word in reviews_dict:\n",
    "    custom_dict.add(word)\n",
    "\n",
    "\n",
    "trie = Trie(custom_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_correction_dict(incorrect_word,corrected):\n",
    "    if type(incorrect_word)==str:\n",
    "        correction_dict[incorrect_word]=corrected\n",
    "    else:\n",
    "        if type(incorrect_word)==tuple:\n",
    "            for word in incorrect_word:\n",
    "                correction_dict[word]=corrected\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_dict={}\n",
    "word_correction_dict(('บ','บอ','บ.'),'บริษัท')\n",
    "word_correction_dict(('call center','Call Center','คอลเซนเตอ','คอลเซนเตอร์'),'คอลเซ็นเตอร์')\n",
    "word_correction_dict(('พนง','Staff'),'พนักงาน')\n",
    "word_correction_dict(('Shoppee','ช็อปปี้','ชอปปี้','shopee','Shoppee','ชอบปี้','shoppee','ช้อปปิ้','platform'),'ช้อปปี้')\n",
    "word_correction_dict(('coin','คอยส์','coins','coincashback'),'คอยน์')\n",
    "word_correction_dict('จนท','เจ้าหน้าที่')\n",
    "word_correction_dict(('แอร์เพ','airpay','แอร์เพย์','Airpays','Airpay','Air pay','air pay','air pays','Airplay','airplay','แอเพลย์','แอร์เพลย์','AirPay'),'แอร์เพย์')\n",
    "word_correction_dict('ตำ','สั่ง')\n",
    "word_correction_dict('แกง','หลอก')\n",
    "word_correction_dict('ดือ','ดี')\n",
    "word_correction_dict('มปร','ไม่เป็นไร')\n",
    "word_correction_dict('ค้าส่ง','ค่าส่ง')\n",
    "word_correction_dict('พน','พรุ่งนี้')\n",
    "word_correction_dict('คสอ','เครื่องสำอางค์')\n",
    "word_correction_dict('ลค','ลูกค้า')\n",
    "word_correction_dict(('เคอรี่','kerry','Kerry'),'เคอร์รี่')\n",
    "word_correction_dict(('Ninjavan','นินจา แวน','ninja van', 'Ninja Van','นินจา','ninja','Ninja','ninjavan'),'นินจาแวน')\n",
    "word_correction_dict(('Shope Express','Shopee express','ShopeeExpress','ช้อปปี้ express','ช้อปปี้ Express','ช้อปปี้เอกเพลส','ช้อปปี้เอ็กเพลส'),'ช้อปปี้เอ็กซ์เพรส')\n",
    "word_correction_dict(('Best Express','Best express','BestExpress','bestexpress','BEST EXPRESS'),'เบสเอกเพลส')\n",
    "word_correction_dict(('flash','Falsh'),'แฟลช')\n",
    "word_correction_dict(('DHL','dhl'),'ดีเอชแอล')\n",
    "word_correction_dict(('ปณ', 'thaipost'),'ไปษณีย์')\n",
    "word_correction_dict(('app'),'แอพ')\n",
    "word_correction_dict(('ม๊วก','มั๊ก','มั่ก'),'มาก')\n",
    "word_correction_dict(('random','ramdom','แรนด้อม','แรด้อม'),'สุ่ม')\n",
    "word_correction_dict(('WTF','wtf','วดฟ'),'ห่วย')\n",
    "word_correction_dict(('มั่ยดั้ย'),'ไม่ได้')\n",
    "word_correction_dict(('free shipping'),'ส่งฟรี')\n",
    "word_correction_dict(('cashback','Cashback'),'แคชแบค')\n",
    "word_correction_dict(('ขอบคุ'),'ขอบคุณ')\n",
    "word_correction_dict(('กทม'),'กรุงเทพ')\n",
    "word_correction_dict(('Security'),'ความปลอดภัย')\n",
    "word_correction_dict(('Live','live','LIVE'),'ไลฟ์')\n",
    "word_correction_dict(('Preorder','Pre Order'),'พรีออเดอร์')\n",
    "word_correction_dict(('เสิร์จ','search','เสิช','เสิ้จ','เสิร์ท'),'ค้นหา') \n",
    "word_correction_dict(('delivery','shipping'),'ขนส่ง')\n",
    "word_correction_dict(('ปสด'),'ประสาทแดก')\n",
    "word_correction_dict(('เคียด'),'เครียด')\n",
    "word_correction_dict(('โค้ดลับ','Code','code','โค๊ด','โค้ต','โค้ลลด'),'โค้ด')\n",
    "word_correction_dict(('เวป','เว็บ','web','เว็ป'),'เว็บไซต์') \n",
    "word_correction_dict(('email','เมลล์','mail','อีเมล','เมล์'),'เมล')\n",
    "word_correction_dict(('love','Love','รั้ก'),'รัก')\n",
    "word_correction_dict(('happy','แฮปปี้','Happy'),'มีความสุข')\n",
    "word_correction_dict(('link','Link','ลิ้ง','ลิ้งปุ้'),'ลิ้งค์')\n",
    "word_correction_dict(('วอลเลท','wallet','วอเลต','wallet'),'วอลเล็ต')\n",
    "\n",
    "def preprocessor(sentence):\n",
    "    \n",
    "    #remove hashtag\n",
    "    hashtag_removed = re.sub(r\"#\\w+\",'', sentence) \n",
    "    \n",
    "    #remove url\n",
    "    url_removed = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','', hashtag_removed)\n",
    "\n",
    "    stop_words = list(pythainlp.corpus.thai_stopwords())\n",
    "    remove_words = ['นะค่ะ','นะคะ','ๆๆ','ๆ','ๆๆๆ','หรอ','งะ','เตง','วะ','อ่ะ','เออ','อะ','อ่า',\n",
    "              'นั้นแหละ','เหรอ','กก','ตะ','กะ','อะ','เปน','ที้ศื้อ','ก้','เดี๋ยวนี้','5000','500','11','39',\n",
    "               'ว่ะ','TvT','นัง','เด้อ','ก้อ','กุ','ค้าบ','90','การ','แทบจะ','โดยที่','10','200','เอาละ','special','อิ',\n",
    "               'เดือน','350','ฟุต','รอบ','รีด','169','150', '175', 'vogue','งี้', 'จก', 'จิงๆ','3000', '40', '4900', '50', \n",
    "               'clarins', 'morebeauties','แทบ','100', '300', 'flower','ซ้ำ','มุง','cpjgirlxx', 'คุณหนู','ฟี', 'มู้ด',\n",
    "               'ออ','199', 'cicishop','เดอะ', 'เบส','เดอะ', 'เบส','ทุ','แล้วจะ','ง่ะ','หก','เค้า','แห','ตั่ง','มค', 'ยัน',\n",
    "               'คา', 'ตอนนี้', 'ตะหาก','อะนะ', 'เย', 'แบ่บ','30', 'lover','ไหนจะ', 'case', 'ex','ยัย','สะ',\n",
    "               'คัด', 'ชิค', 'ฝอ','ววว','โล่', '399', 'pnjzyn', 'shoes', 'ก๊อกแก๊ก', 'คู่', 'จา','สนีก','อร', 'เก','99',\n",
    "               'มั้ง', 'เนี้ย','76', 'cotton','นา', 'ปัก','12','120','รต','รี่','เค','เพล', 'เอก', 'แก้', 'โช', '900',\n",
    "               'ซิ','เด', '69', 'กระจูด','Lock', 'amp','89', 'lemon', 'vibes','นะค้า','Roushun', 'Some', 'by', 'mi',\n",
    "               '800', 'ชี', 'นึง','490', 'divoom','ป่านนี้','ลาซาด้า','60', 'เธรด', '498','thread','everyday', 'look', \n",
    "               'น๊า','299','180', '29','36', '53', 'cm','600', 'ml','จ่ะ','Xiaomi','ปะ','แอยย','15','539', 'Rose',\n",
    "               '41','air', 'deerma', 'humidifier', 'xiaomi','20','ดิ','1200', 'official','119','อีฉัน', '3700', 'about', \n",
    "               'airpods', 'vickyshop','553','แฮะ','655','159', '78','45', '24', 'Centellian','on', 'sale', 'top',\n",
    "               '250', 'bentoy','vibe','Lazada','อี','booster', 'clear', 'nct', 'sm', 'sp','149', 'Plantnery', 'tea', \n",
    "               'tree', 'ๆๆ','CHEF', 'DREAM','grafen', 'yvescosmetic','503', '650', 'Klinn', 'Oxygen', 'diffuser', 'room',\n",
    "               'ฮะ','CJ', 'HYA', 'Vit', 'supermarket','dashing', 'diva', 'fit', 'slim', 'super','ทันใด','แหม', 'โห้ๆๆ', 'โอ',\n",
    "               'ๆผ', 'partner','พรีออเดอร์', '01', '08', '09', '22', '588', '699','gt','000', 'unpack',\n",
    "               'wfh','david', 'flash', 'jones','koreatownofficial','earbud', 'soundpeats', 'trueair','งับ','ค่าเธรด',\n",
    "               '573', '95', 'HJV','ง่า','25', 'Bears', 'Care','25', 'ขายของ', 'Bears', 'Care','ศศ','ลซด','Davidjones', \n",
    "               'แน่ๆ','294', '49', 'หมอน', 'bulky','ล่ะ', 'อ๊าว','tokpokki','449','46', 'start','Allie', 'Official', 'lt',\n",
    "               '990', 'Bud', 'Pro', 'Samsung', 'Store', 'Website','27','ยยยย','beauty', 'multy', '167', '23',\n",
    "               'Facebook', 'Morglorythailand', 'ORI', 'Page','2500','Mobile', 'WPN', 'mbk','55555', '2700', '880',\n",
    "               'Argan', 'Elegance', 'Hair', 'Oil', 'เอย', 'Select', 'Shop', 'Bulky','APEX', 'KOREA', 'konvy',\n",
    "               'account', 'sappun', 'cj', 'logistics', 'เอิ่ม', 'เอ่ย', '101', 'Cezanne', 'best', 'the', '34',\n",
    "               'Normal', 'Standard', '26','หรอก', '55', '74', 'qqaukbj','mini', 'projector','BULKY','172'\n",
    "               '400', 'FBSH','เอ้ย', 'เด่', 'เรย', 'POS','อีฟ', 'Bra', 'Jutui', '197', 'NATPATTY', '674','dm',\n",
    "               '15000', '318', 'Li', 'Net', 'Power', 'RPP', 'Remax', 'Type', 'Weight', 'bank', 'battery', \n",
    "               'iPhone', 'mAh', 'polymer', '850', '160', 'jt','555555555', '827','569', '70', '770', '790', 'Border',\n",
    "               'Day', 'Dm', 'One', 'POSTER', 'DM', 'Pt','5555555', 'คิคิ', '02', '21', 'nacific', 'out', 'sold',\n",
    "               '290', '320', 'Jackson', 'โอ้โห', '195', '390', 'CK', '35', '450', 'City', 'Marvelous', 'Size', 'fanmade',\n",
    "               'ๆๆไๆไ', 'วววว', 'ไต', '165', 'ๆๆๆๆๆ', '1700', 'hitachi', 'sharp', '00', '04', 'fee', 'ลทฟอม', 'buy', 'power',\n",
    "               'Hive', 'Rhythm', '630','77', '79', '394', 'KUNYA', '14', '18', 'Best', 'อุ', 'อ๋อ', '37', 'Stock', 'bio'\n",
    "               ,' ', 'TT', '4000', '196','ก', '140', '189', 'Later', 'Next', 'DairyHome', 'Inter', '925', '2020', '380', 'FIGARO', \n",
    "               'Japan', 'Madame', 'Nendoroid', 'วาก', 'BSWBD', 'readlenlen','Etude', '690', '188', 'flip', 'วอ', 'หว่า',\n",
    "               'ฮ', '214', 'daily', 'prize', '4.4','0.1', '0.2', '0.5','1300', '555555', 'pro', 'ultra', '1450', '55555555', 'chanel',\n",
    "               '7732', '135','netflix','ไอจี','ig','นี้แหละ','แล้วจะ','ที่จะ','วว', 'หวีด','Market', 'place','ไหม',\n",
    "               'เลย','สักวัน','พับจี','เล๊ย', 'พุ่ง','โด้', 'ดึง','ๆๆๆๆ','platinum','joox', 'rov', 'spotify','109','เซฟอล่า','325',\n",
    "               'cargo', 'tag', 'taobao', 'ป่ะ', 'อีกแล้ว','เนเจอร์','แมสก์','อยู่แล้ว', 'อิอิ','เตง','แล้วด้วย','TH','อม','22','โคตร',\n",
    "                'วว', 'อาโป','ตปท','แพลตินัม','ค่าบริการ','พับจี','เล๊ย','อีกครั้ง','sms','มามามู','อันนี้', 'เมื่อกี้','หน็อย','ไลน์', '17', 'กี', 'ค้า', 'ทท', \n",
    "                'นนน','หละ','อิ','งั้น','งั้น', 'สัก','แหม่','สส','แพทตินั่ม','Delivery', 'Same','พี่เฟย์', 'ปป', 'drop', 'mic','Link','ดดดด',\n",
    "                'โลด','order','แล้วก็','IG', 'save', 'อีกที','เอะอะ','lazada', 'closed','2021','บบ', 'CP','โก', 'ก้า', 'มูส','ถามติง','จิง',\n",
    "                'เห้ย', 'redeem','เดย์', 'ชม.', 'แป๊ปๆ','อยู่เลย', 'Apple','อิน', 'เจ้','นัท', '5555','ฮาวทู','นน', 'อืม', 'ว่าซั่น', 'เนี่ยะ','แล้วไป',\n",
    "                'มัวแต่', 'เซน', 'เต้อ', 'เวน', 'ดัน','สิ','สู', 'ซิลเวอร์', 'นัม','แพลตินัม','โถ่','ว่าแต่','เนอะ','อืน', 'อ่อ','ก๊อ','ยยย', 'professional',\n",
    "                'whoami', 'กุแหละ', '555','เหอะ', 'Platinum', 'Ac', 'Nattamon', 'natt','SKYWORTH', 'skyworth', 'คลิ้ก','gold', 'member','ฮู้', \n",
    "                '30000', 'Gold','ร๊ยย', 'วน', 'availability','1100', '1900','อิเกีย','48', 'รป', 'เห้อ', 'หึ','day', 'off','อีกด้วย','ไบร์ทมิวซ์','ยย',\n",
    "                'ดรีม','เวย์วี','เดะ', 'เด่ะ','โอ้ย', '11000','ติ้กตอก', 'ต่ะ','ป๋ม','โว้ย', 'wdm','52', '54','อยุ่ต้ะ','พก', 'ฟิวส์', 'ก้าด', 'พาหุรัด', 'สยาม', 'เฟรม', \n",
    "                'เวิร์ล', 'ไชน่า', 'said', 'เซ้น','ซัพน้องอี้ป๋อ','มุ้ยคั้บ', 'pika', 'มาร์ค','มั่ง','warota', 'ฟตซ','ว้ะ', 'เต๊าะ','candylab',\n",
    "                'มมม','น้ออ','dhl','เฮ้ย','ดด', 'มะ', 'วาน', '65','อาทิต','ป้ะ','มึ้งง', 'ย่ะ', '64', 'เอ่อ', '127', '205', 'อิบอพ','104', 'Card', 'Cashbee', \n",
    "                'น๊อ','อี๊', 'ems','EMS','SuperM','dvd', '115', 'เงี้ย','ลทบ', 'ฟตบ', 'วุย', 'เอ้อ', '19', 'ได่','no' ,'เอ๊ย','เอ้า','expless', 'นนนน',\n",
    "                '28', 'อ้าว', 'เซิร์ช', 'กค', 'คร๊', 'ร๊', 'ากกกกก', '', '80', 'ป๊ะ', 'ถถถ','หว่ส' ,'gif', 'jd','ฯ','ไม๊', 'Act', 'Again', 'Album', 'Dance', \n",
    "                'Gonna', 'Never', 'TAEMIN', 'The', 'rd', 'พิน', '탬니', 'studio', 'คั้บ', 'เน่อ', 'เบย', 'อ้ย', 'โป้', 'โฮป', 'chapter', 'mbookstoreshop', 'one'\n",
    "                , '42', 'io', 'แหะ', '88', 'นว', 'เน้าะ', 'ซอก', 'อู',' 85.', 'wabelle', '13','chocolate', 'ice', 'pack', '75', 'เนี่น','votive',\n",
    "                '59','JT','makeup', 'please', '750', 'มมมม', '47', 'adidas', 'kratop', 'Drop', 'Off','ก้', 'แน้ว', 'PVC', 'ล้ะ', '385', 'STD', '16',\n",
    "                 'Answer', 'is', 'unacceptable','WFH','1414', '1500', '1530', '170', '72','225', 'return', '130', 'มั๋ย', '2564', '110', '190', 'นนนนนน'\n",
    "                 , 'Xpres', 'shein', 'Gateway', 'ยยยยย', 'update', 'Only', 'at', 'ARM', 'FRP', 'FSTRMAR', 'Discount', 'Max', 'Min', 'WCSPORT',\n",
    "                  'Jamclub', 'Jamshop', 'Sale','มังงะ', 'มังฮวา', 'ว้อย', 'qr', 'ยยยยยย', 'Fabricposter', 'timeline', 'ช้อง', 'ยยยยยยยยย' , 'diy', 'house',\n",
    "                 'อ้ะ', 'Application','จวพ','56', 'ว้อททท', 'พค', 'ง.','.', 'เอ๊ะ', 'pt', '950', 'mandalaki', '1600', '1651', '400', '51', 'Classic', 'Silver',\n",
    "                  'กะรัต', '270', 'sliver', 'LAGOM','spay','อฟซ','me', 'remind','เนาะ','700','กก','SMASOCIAL', 'mall', 'ญญ', 'ฟพด', 'มุ', 'แมน', '31', 'ด์', \n",
    "                  'innisfree', 'ฟฟ','07','179', 'FASH', 'Season', 'TR', 'sweater', 'Rewards', 'น้องเบ', 'Ig', 'doubletttt', '️️', 'RE', 'Freesize', 'Tiny',\n",
    "                  '155','฿','FASH2TR','199', 'ANGELS', 'FENNEC', 'STRETCH', 'follow', 'บัก', 'อักษร', 'downy', 'tinytan', 'man', 'peter', 'unleashia', 'ทิ้นท์',\n",
    "                  'finnomena','จจจ', 'shopaholic', 'จว.', '405', '499', 'Wetv', 'expo', 'gadget', 'smartphone', 'งม', '224', 'ABIDE', '277', 'NADIA',\n",
    "                  'name','cf', 'Value', 'Dtac', 'Clarins', 'Galaxy', 'Smart', 'TV', 'sephora', 'เล', '', 'ecommerce','NCT', 'งื้ออ', 'mizumi',\n",
    "                   'ดดดดด','ลาซาด้า','น้องดรีม','อซท','applesheep', 'ยยยย้', 'ขข', 'เหม่', '590', '495', 'Watsons','550', '106', 'zoflora', 'Shopback',\n",
    "                  'ห้ะ','Brand','AirPods','2000','minimall','usb','xGOT', 'โบ๊ะบ๊ะ', '172','butterscotch', 'mart','เปน', 'ak', 'annywawony', 'line', 'outlet', 'th'\n",
    "                  'youus', '2790','Nintendo', 'Switch','44','BNK', 'Photoset', 'Up', 'scale','เห้ยย', '304', '419', 'CC', 'PA', 'SPF', 'Sunscreen', 'Tinted', 'Zunshield',\n",
    "                  'AA', 'Line', 'website', 'LZD', 'ssktmmee', 'sticker', 'tiny', '˃̵͈̑ᴗ˂̵͈̑', 'Airpods', '29','295', 'Ems', 'แจบ', 'acc', 'schwnn','Oppo', 'Reno', '1000', '85'\n",
    "                  , 'FB','1340', 'FOUNDATION', 'LONG', 'SKIN', 'WEAR', 'WEIGHTLESS', 'bobbi', 'brown', '2022', '202', 'Heroine', 'make', 'mascara', 'speedy',\n",
    "                  '279','ALBUM', 'ATEEZ', 'FEVER', 'MINI', 'ORDER', 'PRE', 'Part', 'ZERO', 'ver', 'boxbox', 'Oxe', 'acne', 'cure', 'lotion', 'powder',\n",
    "                  'Balm', 'HOrME', 'Relaxing', 'aroma','china', 'in', 'made', 'lilybyredthailand', 'blu', 'ray','Two', 'two','Bio','Momsta', 'dicon','beyondsoho',\n",
    "                  '126', '2017', 'riverside','38', 'manyo', 'low', 'stock', '230', '345','นิ้เบย','134', '206', 'Post', 'it', 'toxic', 'Marhen', 'เบยยย',\n",
    "                  'index', '1490', 'remax', 'apple', 'music', 'MBK', 'sony', 'Dairy', 'Home','insalon','naimx','Marimekko','ipad','178','91','silver',\n",
    "                  'everY', 'eloop','Lyrics', 'BB', 'MT', 'YG', '1690', 'hi', 'jet', 'melon', 'given', 'electrolux', 'kg', 'BOL', 'synnara','loft','face',\n",
    "                  '730', 'Arrival','โยชิ','811','lomo', 'หว่ะ', '108', 'viu', 'wetv','1360','xxx', 'Sulwhasoo','sos', 'isse', 'miyake','นน','facebook', 'twitter',\n",
    "                  'บร่ะ','ยยยยยย','ยยยย', 'kookshop', 'kr','nd', 'nude', 'rom', 'shell', '176', 'size','129', 'Shein', 'US', 'beautysite', 'international', 'norino', 'XL',\n",
    "                  '58','CE', '5555555555555', 'ads','ทะ', '1150', '3800','น', '217','207', 'roomieshobby','Fatal', 'Monsta','Holika', 'holika','qc',\n",
    "                  'Uki', 'stationery', 'rm', 'อุ๊ย', '43','ลาซาด้า', 'xl','ออฟฟิเชี่ยล','Member','ๆๆๆๆๆๆ','ok', '510', 'Midnight', 'AtrPay','ap',\n",
    "                  'refund','UX','762', 'if', 'then', 'banking','mamonde', 'Agoda', 'scb','220', '1042','222', '446', 'ก้บั่บ', 'Iam','BTS', 'google', 'แง่ง', 'โด้ย',\n",
    "                  'งื้อ','Scb','Update', 'later', 'electronics','2800','พิ้หมิวๆๆ','ป้ะ?', 'list', 'wish', 'BCURU', 'LVBUE', 'adapter', 'lightning', 'padair', 'type',\n",
    "                  'week','ยูทูป','แอนด์','จ๋วย']\n",
    "    for w in remove_words:\n",
    "        stop_words.append(w)\n",
    "\n",
    "    \n",
    "    screening_words = stop_words + remove_words\n",
    "    \n",
    "    merged = ''\n",
    "    #words = pythainlp.word_tokenize(str(sentence), engine='newmm')\n",
    "    # sentence = sentence.replace(':','')\n",
    "    words = set(thai_words()) # thai_words() returns frozenset คำที่ีใช้\n",
    "    tweet_words = ['ช้อปปี้','ช็อปปี้','แอร์เพ','ชอปปี้','แรนดอม','ทวิต','วอเลต','โดน','แอร์เพย์','เยอะ','คอยส์','คอยน์','ใช้','ไม่ได้',\n",
    "                  'ไอจี','สักที','เยอะ','แพนิค','Call Center','เอิร์ธโทน','ประทับใจ','แม่ค้า',\n",
    "                   'ยอดลดลง','เกาหลี','ส้นหนา','ยาวดี','จ้อจี้','บลูธูท','แพสชั่น','ติ๊กต๊อก','มินิมอล','สำอางค์','อัลบั้ม','ตีมือ'\n",
    "                   ,'ทิชชู่','เซรั่ม','ไฮยาลูรอน','ฟิน','มูฟออน','หนึบหนับ','คุมะมง','ออฟฟิเชียว', 'โจมาโลน', 'อิดอก', 'เอ็กเพลส',\n",
    "                   'อัลมอน', 'ปุ๊ปปั๊บ', 'แล้วจะ','โควท','หม้อทอดไร้น้ำมัน','อัติโนมัติ','เซนต์เปอร์','ประจำ','พนักงาน','แอดมิน','ติดต่อ','ธนาคาร',\n",
    "                   'ลาซาด้า','ผู้ถือหุ้น','สูญเสีย','ตกต่ำ','ค่าธรรมเนียม','รีทวิต','ติดต่อ','สิงคโปร์','ไทยแลนด์','เลื่อมล้ำ','โฟโต้การ์ด',\n",
    "                   'หาไม่เจอ','เจ้าหน้าที่','ช้อปปี้', 'แอดมิน', 'คอลเซ็นเตอร์','บริษัท', 'ค่าบริการ' ,'รับผิดชอบ', 'ติดต่อ' ,'โกงเงิน', 'บัญชี', 'ดำเนินการ', 'แบ่งชนชั้น', \n",
    "                   'คุยกับพนักงาน', 'ติดต่อ', 'บริการ', 'แจ้ง', 'ระดับ', 'พรีเมียม', 'เมมเบอร์', 'แบน', 'เลิกใช้', 'สื่อสาร', 'ร้องเรียน', 'เลิกช้อป', 'เว็บไซต์', 'เบอร์', 'โทร', \n",
    "                   'เมล', 'สนใจ', 'เพิกเฉย', 'รอสาย', 'ตัดสาย', 'ด่วน', 'เมล', 'โทร', 'ศูนย์บริการ', 'ปัญหา', 'ชดเชย', 'องค์กร', 'เลิกสั่ง' \n",
    "                   'ประทับใจ', 'แนะนำ', 'ชี้แจง', 'นโยบาย', 'สมาชิก', 'การตลาด', 'โฆษณา', 'ลูกค้า', 'ผู้ใช้บริการ', 'แพลตฟอร์ม', 'โฆษณา',\n",
    "                   'ไม่ติดต่อ', 'ยกเลิก', 'รอ', 'ช้า', 'นาน', 'ชั่วโมง', 'ไม่ได้รับ', 'ไม่ได้สินค้า', 'ปัญหา', 'ผู้ส่ง', 'พนักงาน', 'ขนส่ง', 'รอนาน', \n",
    "                   'บริการ', 'ไม่โทรมา','ส่ง','ไม่มาส่ง','ไม่ส่ง', 'สุภาพ', 'บอกทาง', 'เชค','เช็ค','ตรวจ','ที่อยู่', 'ส่งช้า', 'ส่งเร็ว', 'ส่งไว', \n",
    "                   'โทรมา', 'ไม่โทร', 'ไม่รับ', 'เลือกขนส่ง', 'เลือกขนส่งไม่ได้', 'เลือกบริษัทขนส่ง', 'ได้ของ', 'ส่งของ', 'กำหนด', 'บริษัทขนส่ง', 'ค่าส่ง', \n",
    "                   'เก็บเงินปลายทาง', 'เก็บปลายทาง', 'เลขพัสดุ', 'ค่าส่ง', 'จ่าหน้า','โค้ด', 'โค๊ด', 'โค้ดลด', 'โค้ดช้อปปี้', 'กดไม่ทัน', \n",
    "                   'ใช้ไม่ทัน', 'ค่าส่ง', 'ลดราคา', 'ส่วนลด', 'โปรโมชั่น', 'คุ้มค่า' ,'ไม่คุ้ม', 'ถูก','โปร', 'โปรช้อปปี้', 'บอกต่อ', 'ใช้ไม่ได้', 'ใช้โค้ดไม่ได้', 'คุณภาพ', 'ใช้ได้', \n",
    "                   'ขั้นต่ำ', 'โค้ดส่งฟรี', 'ส่งฟรี', 'แกง', 'ลูกค้า', 'ใหม่', 'ฟรี', 'แถม', 'เต็ม', 'กด', 'ทัน', 'เที่ยงคืน', 'แฟลชเซลล์', 'เมมเบอร์', 'สิทธิ์', \n",
    "                   'คอยส์', 'แคมเปญ', 'เหรียญ', 'จ่ายบิล', 'ดีล', 'รดน้ำต้นไม้', 'คูปอง', 'กิจกรรม','ช้อป', 'ซื้อ', 'ซื้อมาจาก', 'ร้านในช้อปปี้', 'ไลฟ์', 'ไลฟ์สด', 'ในช้อปปี้', \n",
    "                   'จากช้อปปี้','สั่งช้อปปี้', 'เข้าช้อปปี้', 'ที่ช้อปปี้', 'ราคา', 'คุ้ม', 'คุ้มค่า', 'ไม่คุ้ม', 'แพง', 'ถูก', 'ลด', 'บอกต่อ', 'คุ้ม', 'คุณภาพ', \n",
    "                   'ซื้อ', 'ส่วนต่าง', 'ควรค่า','ควรซื้อ', 'ไปตำ', 'แนะนำ', 'ร้าน', 'ร้านค้า', 'พิกัด', 'ร้าน', 'รีวิว', 'สั่ง', 'สอย', 'ซื้อจาก', 'รวมของที่ซื้อ', 'สินค้า', \n",
    "                   'ของ', 'ขาย', 'ขายของ', 'บาท', 'ใน', 'จาก', 'พร้อมส่ง', 'คำสั่งซื้อ', 'รายงานผู้ใช้', 'แม่ค้า', 'พ่อค้า', 'โกง', 'ผู้ขาย', 'แกง', 'ร้านเสื้อผ้า', \n",
    "                   'เสียเงิน', 'ขายที่', 'ขายดี', 'หมดไว', 'ปลอม','แอพ', 'แอพช้อปปี้','แอพลิเคชั่น', 'หน้าจอ', 'รวน', 'สมัคร', 'ไถ', 'ไถช้อปปี้', 'ชั่วคราว', 'ระบบ', \n",
    "                   'เงิน', 'เว็บ', 'หน้าเว็บ', 'ตัด', 'บัตร','แคนเซิล', 'ยกเลิก', 'เป็นอะไร','พัง', 'ค้าง', 'ล่ม', 'เสีย', 'ไม่ได้', 'เด้ง', 'รอ', 'พัฒนา', \n",
    "                   'หักตังค์', 'หักเงิน', 'คืนเงิน', 'ตัดเงิน', 'ตัดบัตร', 'บัตร', 'หักเงิน', 'ยาก', 'ง่าย', 'พัฒนา', 'ไม่ตรง', 'โอน', 'โอนเงิน', 'เครดิต', \n",
    "                   'คืน', 'แก้', 'ด่วน', 'ขัดข้อง', 'ฟื้น', 'แก้ไข', 'จ่ายเงิน', 'ผ่อน', 'ปัญหา', 'โดนระงับ', 'ตรวจสอบ', 'ขั้นตอน', 'เงินไม่เข้า', 'เติมเงิน', 'ไม่เข้า', \n",
    "                   'จ่ายไม่ได้', 'แบบใหม่', 'แบบเก่า', 'บัตรเครดิต', 'บอท', 'อัพเดต', 'เงินคืน', 'บัญชี', 'อนุมัติ', 'กดเงิน', 'ช้อปปี้เพย์', 'แอร์เพย์', 'จอ', 'อัตโนมัติ', \n",
    "                   'ฟีด', 'โหมดดาร์ก', 'สถานะ', 'แฮ็ค', 'ตะกร้า', 'แจ้งเตือน', 'ระงับ', 'ยกเลิก', 'คำสั่งซื้อ', 'ตรวจสอบ', 'ลิ้งค์', 'สะดวก', 'ระบบล่ม', 'เติม', \n",
    "                   'โหลด', 'ปัดจอ', 'โหลด', 'เสิร์ช' ,'หมวด', 'ธุรกรรม', 'วงเงิน', 'วอลเล็ต','รีฟันด์','ท่องโลก','หลงไหล','เปอร์เซ็น','คนละครึ่ง','วอชเชอร์','บราวนี่',\n",
    "                   'กาแฟ่','เม้าส์','ออฟฟิต','เมนชั่น','โฟโต้ช้อป', 'เฟอร์นิเจอ','จิตใจ','ป้ายยา', 'ปุ๊กปิ๊ก','อินฟลูเซอร์','ระบุ','ขี้งก','ศีลเสมอกัน','เมมเบอร์','คลีนิก','ตาแตก'\n",
    "                    ,'เปียกน้ำ','โลภ','ดีเอชแอล','แฟลช','นินจาแวน','ช้อปปี้เอ็กซ์เพรส','เบสเอกเพลส','เคอร์รี่','ชำระ','ล่ำลือ','สีกรม','ไหนคะ','อิควัย','บัดซบ','แบนอยู่',\n",
    "                   'เปย์','แดรี่โฮม','หมั่นโหนก','กี๊ด','อัดเทป','ฉัน','เธอ','คุณ','เบื่อ', 'ตรุษจีน','เร็ว','ชิบหาย','ชิปหาย','มัดย้อม','โคย', 'แม็กเน็ต', 'อนิเมะ','มังงะ', \n",
    "                   'มังฮวา','สลีปปิ้งมาส์ก','ช้อปลาเนจ','มิสเพริกริน','ไดโซะ','เลิศ', 'ครัวซองค์','ออเดอร์','เดียว','ขนส่ง','แอค','สติ้กเกอร์','กลิ้ตเตอร์','รอยอลคันนิง',\n",
    "                   'เจลว่านหาง','เดรบิต','เหลือ','ช็อปแบค','จน','ราคา','กาตูน','อิโมจิ','บิสกิต','ทิ้ง','เสือก','มิว','ฮีล','อัลมอนต์','ซอส','ป้อบคอร์น','ออฟฟิเชี่ยล',\n",
    "                   'เซิ่นเจิ่น','วอลเลท','กรอง','ห่วยแตก','ห่วย','การันตี','สาระแน','น่ารักมาก','น่ารักจัง','น่ารักจริง','ลดเยอะดี','ไวมาก','มีความสุข','ไปจัด','ดีงาม',\n",
    "                    'ดีงามมาก','ปังจริง','สนใจ','ให้ความช่วยเหลือดีมาก','อิน','เยียวยาจิตใจ','อยากได้หมด','สุดยอด','แอดมินน่ารัก','ไม่โดนโกง','อิสัส','บอกลา','ซื้อด้วยไม่ไหว',\n",
    "                    'โดนหลอก','ทุเรศ','อิดอก','สัส','ช้อปปี้ก็โกง','อย่างเศร้า','เอาเปรียบ','สาระแน','ควย','พนักงานไม่สนใจ','เอาเปรียบ','เจ็บใจ','ตบกับกู','ไม่เข้ารับพัสดุ',\n",
    "                    'อิเวร','ตอแหล','อมเงิน','นังทรยศ','มิจฉาชีพ','โกงเงิน','เสียลูกค้า','ปรับปรุงแล้วแย่','แบน','ลบบริษัทช้อปปี้','พังพินาศ','โกรธบริษัทช้อปปี้','เหม็นหน้า',\n",
    "                    'ไม่ชอบแอดมิน','ชิปหาย','เลิกซื้อ','รีพอท','ห่วยแตก','ห่าเหว','เลิกใช้','บริการไม่ดี','พนักงานแย่','อิเหี้ย','โกง','เกลียดมัน','โคตรแย่','ลาก่อนบริษัทช้อปปี้',\n",
    "                    'เซงมาก','เลิกใช้บริษัทช้อปปี้','เลือกขนส่งไม่ได้','ค่าส่งแพง','งงมาก','อย่าหาทำ','โจร','ใจเกือบพัง','ห่วยแตก','ขนส่งห่วย','เสียความรู้สึก','ระบบสุ่มขนส่ง',\n",
    "                    'แพงจริง','เลื่อนส่งของ','สุ่มขนส่ง','เลือกขนส่ง','สาระแนเลือกขนส่ง','ประสาท','บรรลัย','ค่าส่งแพงเกิน','เลิกสั่ง','ไม่ส่งของ','ค่าส่งแรงมาก','ระบบห่วยแท้',\n",
    "                    'ขนส่งที่ได้แย่มาก','อยากจะด่า','พ่อมึงตาย','เลือกขนส่งเองก็ไม่ได้','สาระแนส่งของ','ห่วยมาก','งงมาก','เป็นเหี้ยไร','ยกเลิกออเดอร์','ตอแหลสถานะ','อิ','เหี้ย',\n",
    "                    'ขนส่งช้ามาก','ขนส่งช้า','อย่าสาระแน','เอาเปรียบ','บ้าบอคอแตก','ห่วยแตกมาก','นานชิบหาย','โคตรห่วย','เหี้ยจริง','อีควาย','ส่งฟรีที่ไม่มีอยู่จริง','อิควาย','ส่งช้ามาก',\n",
    "                    'เลือกขนส่งเองไม่ได้','ขนส่งห่วย','ขนส่งก็แย่','เปลี่ยนระบบขนส่ง','โคตรซวย','อิเวร','อิเวน','เกลียดช้อปปี้','เซ็ง','อิควัย','ไม่มีมารยาท','อยากสั่งของ','เลือกขนส่งไม่ได้',\n",
    "                    ]\n",
    "    for w in tweet_words:\n",
    "        words.add(w)\n",
    "    \n",
    "    #word tokenize\n",
    "    custom_tokenizer = Tokenizer(words)\n",
    "    custom_dictionary_trie = Trie(words)\n",
    "    words = pythainlp.word_tokenize(str(sentence), \n",
    "                                  custom_dict=custom_dictionary_trie, \n",
    "                                  engine='newmm')\n",
    "\n",
    "    words_pos = pos_tag(words,corpus='orchid_ud')\n",
    "    for word in custom_tokenizer.word_tokenize(sentence):\n",
    "        word_norm = normalize(word)\n",
    "        if word_norm in correction_dict:\n",
    "            word_norm = correction_dict[word_norm]\n",
    "        if word_norm not in screening_words and word_norm!='' and len(word_norm)>1:\n",
    "            merged = merged + ',' + word_norm\n",
    "    return merged[1:]\n",
    "\n",
    "    for word in words_pos:\n",
    "        if word[0] not in screening_words:\n",
    "        # print(word)\n",
    "            if word[1] in ['NOUN']: #,'VERB','ADJ','ADV'\n",
    "                if not word[0].isnumeric():\n",
    "                    merged = merged + ',' + word[0]\n",
    "    return merged[1:]\n",
    "    #print('ชอปปี้' in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=preprocessor)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', LogisticRegression()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_text']\n",
    "y = df[['Topics', 'Sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.fit(X_train, y_train['Topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predict = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test['Topics'],y_predict))\n",
    "# print(confusion_matrix(y_test['Topics'],y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Train')\n",
    "# y_train_pred_svm = cross_val_predict(pipeline, X_train, y_train['Topics'], cv=10, n_jobs=-1)\n",
    "# print(classification_report(y_train['Topics'], y_train_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: Company\n",
      "428 428\n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.65      0.70      0.67       159\n",
      "         neu       0.56      0.56      0.56       135\n",
      "         pos       0.64      0.57      0.60       134\n",
      "\n",
      "    accuracy                           0.62       428\n",
      "   macro avg       0.61      0.61      0.61       428\n",
      "weighted avg       0.62      0.62      0.62       428\n",
      "\n",
      "[[111  26  22]\n",
      " [ 37  76  22]\n",
      " [ 24  33  77]]\n",
      "\n",
      "\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.58      0.72      0.64        40\n",
      "         neu       0.62      0.53      0.57        34\n",
      "         pos       0.46      0.39      0.43        33\n",
      "\n",
      "    accuracy                           0.56       107\n",
      "   macro avg       0.55      0.55      0.55       107\n",
      "weighted avg       0.56      0.56      0.55       107\n",
      "\n",
      "[[29  3  8]\n",
      " [ 9 18  7]\n",
      " [12  8 13]]\n",
      "\n",
      "\n",
      "\n",
      "Topics: Logistics\n",
      "897 897\n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.69      0.96      0.80       564\n",
      "         neu       0.56      0.15      0.24       175\n",
      "         pos       0.83      0.34      0.48       158\n",
      "\n",
      "    accuracy                           0.69       897\n",
      "   macro avg       0.69      0.48      0.51       897\n",
      "weighted avg       0.69      0.69      0.63       897\n",
      "\n",
      "[[539  15  10]\n",
      " [147  27   1]\n",
      " [ 99   6  53]]\n",
      "\n",
      "\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.68      0.98      0.80       141\n",
      "         neu       0.55      0.14      0.22        44\n",
      "         pos       0.73      0.21      0.32        39\n",
      "\n",
      "    accuracy                           0.68       224\n",
      "   macro avg       0.65      0.44      0.45       224\n",
      "weighted avg       0.66      0.68      0.61       224\n",
      "\n",
      "[[138   2   1]\n",
      " [ 36   6   2]\n",
      " [ 28   3   8]]\n",
      "\n",
      "\n",
      "\n",
      "Topics: Promotion\n",
      "484 484\n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.61      0.69      0.64       173\n",
      "         neu       0.55      0.68      0.61       181\n",
      "         pos       0.77      0.38      0.51       130\n",
      "\n",
      "    accuracy                           0.60       484\n",
      "   macro avg       0.64      0.58      0.59       484\n",
      "weighted avg       0.63      0.60      0.59       484\n",
      "\n",
      "[[119  50   4]\n",
      " [ 47 123  11]\n",
      " [ 30  51  49]]\n",
      "\n",
      "\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.62      0.53      0.57        43\n",
      "         neu       0.51      0.71      0.59        45\n",
      "         pos       0.67      0.42      0.52        33\n",
      "\n",
      "    accuracy                           0.57       121\n",
      "   macro avg       0.60      0.56      0.56       121\n",
      "weighted avg       0.59      0.57      0.57       121\n",
      "\n",
      "[[23 20  0]\n",
      " [ 6 32  7]\n",
      " [ 8 11 14]]\n",
      "\n",
      "\n",
      "\n",
      "Topics: Store\n",
      "1256 1256\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-d95c2a1ea6a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mvectorizer_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mX_train_raw_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mvectorizer_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1203\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-a2ab3a20e680>\u001b[0m in \u001b[0;36mpreprocessor\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;31m#word tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mcustom_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     \u001b[0mcustom_dictionary_trie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     words = pythainlp.word_tokenize(str(sentence), \n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pythainlp/tokenize/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, custom_dict, engine, keep_whitespace)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__trie_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcustom_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__trie_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_trie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__trie_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEFAULT_WORD_DICT_TRIE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pythainlp/util/trie.py\u001b[0m in \u001b[0;36mdict_trie\u001b[0;34m(dict_source)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# so the Iterable check should be here, at the very end,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# because it has less specificality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mtrie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         raise TypeError(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pythainlp/util/trie.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pythainlp/util/trie.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                 \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mcur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pythainlp/util/trie.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models_raw = {}\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "for topics in ['Company','Logistics','Promotion','Store','System']:\n",
    "    \n",
    "    print('Topics:', topics)\n",
    "    \n",
    "    pipeline_raw = Pipeline([\n",
    "        ('bow', CountVectorizer(analyzer=preprocessor)),  # strings to token integer counts\n",
    "        ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "        ('classifier',OneVsRestClassifier(LogisticRegression()))])  # train on TF-IDF vectors \n",
    "    \n",
    "    X_train_raw = X_train[y_train['Topics'] == topics]\n",
    "    y_train_raw = y_train['Sentiment'][y_train['Topics'] == topics]\n",
    "    print(len(X_train_raw), len(y_train_raw))\n",
    "  \n",
    "    vectorizer_vec = CountVectorizer(analyzer=preprocessor)\n",
    "    X_train_raw_vec = vectorizer_vec.fit_transform(X_train_raw)\n",
    "    \n",
    "    vectorizer_tf = TfidfTransformer()\n",
    "    X_train_raw_tf = vectorizer_tf.fit_transform(X_train_raw_vec)\n",
    "    \n",
    "    #Undersampling\n",
    "    X_train_raw_up, y_train_raw_up = rus.fit_sample(X_train_raw_tf, y_train_raw)\n",
    "#   print(len(X_train_raw_up), len(y_train_raw_up))\n",
    "    \n",
    "    pipeline_raw.fit(X_train_raw, y_train_raw)\n",
    "    models_raw[topics] = pipeline_raw\n",
    "    \n",
    "    y_predict_raw_train = pipeline_raw.predict(X_train_raw)\n",
    "    y_predict_raw = pipeline_raw.predict(X_test[y_test['Topics'] == topics])\n",
    "    \n",
    "    print('train')\n",
    "    print(classification_report(y_train_raw,y_predict_raw_train))\n",
    "    print(confusion_matrix(y_train_raw,y_predict_raw_train))\n",
    "    print('\\n'*2)\n",
    "    \n",
    "    print('test')\n",
    "    print(classification_report(y_test['Sentiment'][y_test['Topics'] == topics],y_predict_raw))\n",
    "    print(confusion_matrix(y_test['Sentiment'][y_test['Topics'] == topics],y_predict_raw))\n",
    "    print('\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: Company\n",
      "428 428\n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.72      0.64      0.67       159\n",
      "         neu       0.56      0.63      0.60       159\n",
      "         pos       0.62      0.62      0.62       159\n",
      "\n",
      "    accuracy                           0.63       477\n",
      "   macro avg       0.63      0.63      0.63       477\n",
      "weighted avg       0.63      0.63      0.63       477\n",
      "\n",
      "[[101  34  24]\n",
      " [ 23 100  36]\n",
      " [ 17  43  99]]\n",
      "\n",
      "\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.66      0.62      0.64        40\n",
      "         neu       0.56      0.53      0.55        34\n",
      "         pos       0.46      0.52      0.49        33\n",
      "\n",
      "    accuracy                           0.56       107\n",
      "   macro avg       0.56      0.56      0.56       107\n",
      "weighted avg       0.57      0.56      0.56       107\n",
      "\n",
      "[[25  5 10]\n",
      " [ 6 18 10]\n",
      " [ 7  9 17]]\n",
      "\n",
      "\n",
      "\n",
      "Topics: Logistics\n",
      "897 897\n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.68      0.66      0.67       564\n",
      "         neu       0.68      0.70      0.69       564\n",
      "         pos       0.73      0.74      0.74       564\n",
      "\n",
      "    accuracy                           0.70      1692\n",
      "   macro avg       0.70      0.70      0.70      1692\n",
      "weighted avg       0.70      0.70      0.70      1692\n",
      "\n",
      "[[371 118  75]\n",
      " [ 93 393  78]\n",
      " [ 80  64 420]]\n",
      "\n",
      "\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.85      0.67      0.75       141\n",
      "         neu       0.31      0.43      0.36        44\n",
      "         pos       0.47      0.62      0.53        39\n",
      "\n",
      "    accuracy                           0.62       224\n",
      "   macro avg       0.54      0.57      0.55       224\n",
      "weighted avg       0.68      0.62      0.64       224\n",
      "\n",
      "[[95 31 15]\n",
      " [13 19 12]\n",
      " [ 4 11 24]]\n",
      "\n",
      "\n",
      "\n",
      "Topics: Promotion\n",
      "484 484\n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.63      0.67      0.65       181\n",
      "         neu       0.57      0.47      0.52       181\n",
      "         pos       0.65      0.72      0.69       181\n",
      "\n",
      "    accuracy                           0.62       543\n",
      "   macro avg       0.62      0.62      0.62       543\n",
      "weighted avg       0.62      0.62      0.62       543\n",
      "\n",
      "[[122  38  21]\n",
      " [ 47  85  49]\n",
      " [ 24  26 131]]\n",
      "\n",
      "\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.56      0.56      0.56        43\n",
      "         neu       0.46      0.40      0.43        45\n",
      "         pos       0.44      0.52      0.47        33\n",
      "\n",
      "    accuracy                           0.49       121\n",
      "   macro avg       0.49      0.49      0.49       121\n",
      "weighted avg       0.49      0.49      0.49       121\n",
      "\n",
      "[[24 15  4]\n",
      " [ 9 18 18]\n",
      " [10  6 17]]\n",
      "\n",
      "\n",
      "\n",
      "Topics: Store\n",
      "1256 1256\n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.58      0.61      0.59       502\n",
      "         neu       0.57      0.51      0.54       502\n",
      "         pos       0.56      0.59      0.57       502\n",
      "\n",
      "    accuracy                           0.57      1506\n",
      "   macro avg       0.57      0.57      0.57      1506\n",
      "weighted avg       0.57      0.57      0.57      1506\n",
      "\n",
      "[[304  95 103]\n",
      " [111 256 135]\n",
      " [105  99 298]]\n",
      "\n",
      "\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.46      0.53      0.49        87\n",
      "         neu       0.61      0.52      0.56       126\n",
      "         pos       0.42      0.44      0.43       102\n",
      "\n",
      "    accuracy                           0.50       315\n",
      "   macro avg       0.50      0.50      0.50       315\n",
      "weighted avg       0.51      0.50      0.50       315\n",
      "\n",
      "[[46 17 24]\n",
      " [22 66 38]\n",
      " [31 26 45]]\n",
      "\n",
      "\n",
      "\n",
      "Topics: System\n",
      "501 501\n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.63      0.68      0.65       227\n",
      "         neu       0.65      0.57      0.61       227\n",
      "         pos       0.70      0.73      0.72       227\n",
      "\n",
      "    accuracy                           0.66       681\n",
      "   macro avg       0.66      0.66      0.66       681\n",
      "weighted avg       0.66      0.66      0.66       681\n",
      "\n",
      "[[154  40  33]\n",
      " [ 60 129  38]\n",
      " [ 32  29 166]]\n",
      "\n",
      "\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.56      0.53      0.54        57\n",
      "         neu       0.44      0.39      0.42        41\n",
      "         pos       0.31      0.41      0.35        27\n",
      "\n",
      "    accuracy                           0.46       125\n",
      "   macro avg       0.44      0.44      0.44       125\n",
      "weighted avg       0.47      0.46      0.46       125\n",
      "\n",
      "[[30 11 16]\n",
      " [17 16  8]\n",
      " [ 7  9 11]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models_raw = {}\n",
    "smote = SMOTE()\n",
    "\n",
    "for topics in ['Company','Logistics','Promotion','Store','System']:\n",
    "    \n",
    "    print('Topics:', topics)\n",
    "    \n",
    "    pipeline_raw = Pipeline([\n",
    "        ('bow', CountVectorizer(analyzer=preprocessor)),  # strings to token integer counts\n",
    "        ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "        ('classifier',OneVsRestClassifier(LogisticRegression()))])  # train on TF-IDF vectors \n",
    "    \n",
    "    X_train_raw = X_train[y_train['Topics'] == topics]\n",
    "    y_train_raw = y_train['Sentiment'][y_train['Topics'] == topics]\n",
    "    print(len(X_train_raw), len(y_train_raw))\n",
    "  \n",
    "    vectorizer_vec = CountVectorizer(analyzer=preprocessor)\n",
    "    X_train_raw_vec = vectorizer_vec.fit_transform(X_train_raw)\n",
    "    \n",
    "    vectorizer_tf = TfidfTransformer()\n",
    "    X_train_raw_tf = vectorizer_tf.fit_transform(X_train_raw_vec)\n",
    "    \n",
    "    #Undersampling\n",
    "    X_train_raw_up, y_train_raw_up = smote.fit_sample(X_train_raw_tf, y_train_raw)\n",
    "#   print(len(X_train_raw_up), len(y_train_raw_up))\n",
    "    \n",
    "    model_raw = OneVsRestClassifier(LogisticRegression())\n",
    "    model_raw.fit(X_train_raw_up, y_train_raw_up)\n",
    "    \n",
    "#   pipeline_raw.fit(X_train_raw_up, y_train_raw_up)\n",
    "    models_raw[topics] = model_raw\n",
    "    \n",
    "    X_test_raw_vec = vectorizer_vec.transform(X_test[y_test['Topics'] == topics])\n",
    "    X_test_raw_tf = vectorizer_tf.transform(X_test_raw_vec)\n",
    "    \n",
    "    y_predict_raw_train = model_raw.predict(X_train_raw_up)\n",
    "    y_predict_raw = model_raw.predict(X_test_raw_tf)\n",
    "    \n",
    "    print('train')\n",
    "    print(classification_report(y_train_raw_up,y_predict_raw_train))\n",
    "    print(confusion_matrix(y_train_raw_up,y_predict_raw_train))\n",
    "    print('\\n'*2)\n",
    "    \n",
    "    print('test')\n",
    "    print(classification_report(y_test['Sentiment'][y_test['Topics'] == topics],y_predict_raw))\n",
    "    print(confusion_matrix(y_test['Sentiment'][y_test['Topics'] == topics],y_predict_raw))\n",
    "    print('\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
